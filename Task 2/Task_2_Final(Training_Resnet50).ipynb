{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 18385.734931,
      "end_time": "2021-09-28T18:43:31.533544",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-09-28T13:37:05.798613",
      "version": "2.3.3"
    },
    "colab": {
      "name": "Task-2 Final(Training-Resnet50).ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b642eded"
      },
      "source": [
        "### Task 2"
      ],
      "id": "b642eded"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:12.779033Z",
          "iopub.status.busy": "2021-09-28T13:37:12.777562Z",
          "iopub.status.idle": "2021-09-28T13:37:12.855038Z",
          "shell.execute_reply": "2021-09-28T13:37:12.854487Z",
          "shell.execute_reply.started": "2021-09-28T08:47:06.781706Z"
        },
        "papermill": {
          "duration": 0.104137,
          "end_time": "2021-09-28T13:37:12.855202",
          "exception": false,
          "start_time": "2021-09-28T13:37:12.751065",
          "status": "completed"
        },
        "tags": [],
        "id": "bdbf2279"
      },
      "source": [
        "import os"
      ],
      "id": "bdbf2279",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:12.900116Z",
          "iopub.status.busy": "2021-09-28T13:37:12.899291Z",
          "iopub.status.idle": "2021-09-28T13:37:12.902594Z",
          "shell.execute_reply": "2021-09-28T13:37:12.903008Z",
          "shell.execute_reply.started": "2021-09-28T08:47:06.788785Z"
        },
        "papermill": {
          "duration": 0.026574,
          "end_time": "2021-09-28T13:37:12.903159",
          "exception": false,
          "start_time": "2021-09-28T13:37:12.876585",
          "status": "completed"
        },
        "tags": [],
        "id": "d51d4064"
      },
      "source": [
        "path = \"../input/cnn-data/output\""
      ],
      "id": "d51d4064",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:12.950085Z",
          "iopub.status.busy": "2021-09-28T13:37:12.949580Z",
          "iopub.status.idle": "2021-09-28T13:37:12.975108Z",
          "shell.execute_reply": "2021-09-28T13:37:12.975596Z",
          "shell.execute_reply.started": "2021-09-28T08:47:06.797189Z"
        },
        "papermill": {
          "duration": 0.049561,
          "end_time": "2021-09-28T13:37:12.975776",
          "exception": false,
          "start_time": "2021-09-28T13:37:12.926215",
          "status": "completed"
        },
        "tags": [],
        "id": "9b08f9df"
      },
      "source": [
        "labels =os.listdir(path) #Getting all tha labels"
      ],
      "id": "9b08f9df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:13.044850Z",
          "iopub.status.busy": "2021-09-28T13:37:13.044329Z",
          "iopub.status.idle": "2021-09-28T13:37:14.897157Z",
          "shell.execute_reply": "2021-09-28T13:37:14.896232Z",
          "shell.execute_reply.started": "2021-09-28T08:47:06.821186Z"
        },
        "papermill": {
          "duration": 1.885407,
          "end_time": "2021-09-28T13:37:14.897304",
          "exception": false,
          "start_time": "2021-09-28T13:37:13.011897",
          "status": "completed"
        },
        "tags": [],
        "id": "610e70df"
      },
      "source": [
        "d = {}\n",
        "for x in labels :\n",
        "    d[x] = len(os.listdir(path+'/'+ x)) # No of images in each label"
      ],
      "id": "610e70df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:14.947098Z",
          "iopub.status.busy": "2021-09-28T13:37:14.945924Z",
          "iopub.status.idle": "2021-09-28T13:37:14.949320Z",
          "shell.execute_reply": "2021-09-28T13:37:14.949711Z",
          "shell.execute_reply.started": "2021-09-28T08:47:06.850227Z"
        },
        "papermill": {
          "duration": 0.031131,
          "end_time": "2021-09-28T13:37:14.949835",
          "exception": false,
          "start_time": "2021-09-28T13:37:14.918704",
          "status": "completed"
        },
        "tags": [],
        "id": "beff9914",
        "outputId": "d8e76a12-331c-492d-b438-fee74d2321cd"
      },
      "source": [
        "len(d) #Total nuumbber of classes"
      ],
      "id": "beff9914",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:15.036634Z",
          "iopub.status.busy": "2021-09-28T13:37:15.034184Z",
          "iopub.status.idle": "2021-09-28T13:37:15.039802Z",
          "shell.execute_reply": "2021-09-28T13:37:15.039091Z",
          "shell.execute_reply.started": "2021-09-28T08:47:06.861037Z"
        },
        "papermill": {
          "duration": 0.054625,
          "end_time": "2021-09-28T13:37:15.039974",
          "exception": false,
          "start_time": "2021-09-28T13:37:14.985349",
          "status": "completed"
        },
        "tags": [],
        "id": "cccc2fd5",
        "outputId": "3476095c-d83f-4b9c-f2c1-cab69515df5e"
      },
      "source": [
        "d"
      ],
      "id": "cccc2fd5",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'27.0': 349,\n",
              " '2.0': 83,\n",
              " '8.0': 350,\n",
              " '13.0': 92,\n",
              " '21.0': 349,\n",
              " '20.0': 350,\n",
              " '14.0': 37,\n",
              " '16.0': 86,\n",
              " '12.0': 43,\n",
              " '3.0': 73,\n",
              " '24.0': 60,\n",
              " '15.0': 31,\n",
              " '17.0': 27,\n",
              " '9.0': 141,\n",
              " '0.0': 68,\n",
              " '6.0': 157,\n",
              " '11.0': 70,\n",
              " '28.0': 138,\n",
              " '19.0': 140,\n",
              " '5.0': 80,\n",
              " '7.0': 71,\n",
              " '22.0': 109,\n",
              " '1.0': 73,\n",
              " '4.0': 350,\n",
              " '23.0': 69,\n",
              " '25.0': 155,\n",
              " '26.0': 10,\n",
              " '18.0': 41}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:15.123731Z",
          "iopub.status.busy": "2021-09-28T13:37:15.121820Z",
          "iopub.status.idle": "2021-09-28T13:37:19.869929Z",
          "shell.execute_reply": "2021-09-28T13:37:19.868991Z",
          "shell.execute_reply.started": "2021-09-28T08:47:06.872060Z"
        },
        "papermill": {
          "duration": 4.791428,
          "end_time": "2021-09-28T13:37:19.870106",
          "exception": false,
          "start_time": "2021-09-28T13:37:15.078678",
          "status": "completed"
        },
        "tags": [],
        "id": "ffae497e",
        "outputId": "24d72996-a599-4833-d8db-ad1c11ba0470"
      },
      "source": [
        "# Importing all the required libraries and modules\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n"
      ],
      "id": "ffae497e",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-09-28 13:37:15.639107: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:19.918561Z",
          "iopub.status.busy": "2021-09-28T13:37:19.917770Z",
          "iopub.status.idle": "2021-09-28T13:37:19.920681Z",
          "shell.execute_reply": "2021-09-28T13:37:19.920199Z",
          "shell.execute_reply.started": "2021-09-28T08:47:06.879756Z"
        },
        "papermill": {
          "duration": 0.028658,
          "end_time": "2021-09-28T13:37:19.920787",
          "exception": false,
          "start_time": "2021-09-28T13:37:19.892129",
          "status": "completed"
        },
        "tags": [],
        "id": "69f7b866"
      },
      "source": [
        "\n",
        "filenames = os.listdir(\"../input/cnn-data/output\") # Loading files name \n"
      ],
      "id": "69f7b866",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:19.969740Z",
          "iopub.status.busy": "2021-09-28T13:37:19.968956Z",
          "iopub.status.idle": "2021-09-28T13:37:19.988411Z",
          "shell.execute_reply": "2021-09-28T13:37:19.987946Z",
          "shell.execute_reply.started": "2021-09-28T08:47:06.888850Z"
        },
        "papermill": {
          "duration": 0.045869,
          "end_time": "2021-09-28T13:37:19.988524",
          "exception": false,
          "start_time": "2021-09-28T13:37:19.942655",
          "status": "completed"
        },
        "tags": [],
        "id": "2ede6604"
      },
      "source": [
        "# Creating a data frame\n",
        "\n",
        "files = []\n",
        "categories = []\n",
        "for filename in filenames:\n",
        "    \n",
        "    file  = \"../input/cnn-data/output/\" + filename\n",
        "    \n",
        "    for x in os.listdir(file):\n",
        "        p = file +\"//\"+ x\n",
        "        files.append(p)\n",
        "        categories.append(filename)\n",
        "#         print(filename,p)\n",
        "    \n",
        "df = pd.DataFrame({\n",
        "    'filename': files,\n",
        "    'category': categories\n",
        "})"
      ],
      "id": "2ede6604",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:20.040237Z",
          "iopub.status.busy": "2021-09-28T13:37:20.039452Z",
          "iopub.status.idle": "2021-09-28T13:37:20.053722Z",
          "shell.execute_reply": "2021-09-28T13:37:20.053279Z",
          "shell.execute_reply.started": "2021-09-28T08:47:06.914038Z"
        },
        "papermill": {
          "duration": 0.043353,
          "end_time": "2021-09-28T13:37:20.053843",
          "exception": false,
          "start_time": "2021-09-28T13:37:20.010490",
          "status": "completed"
        },
        "tags": [],
        "id": "65826893",
        "outputId": "2ff421de-be25-4910-f67e-b144a2278aed"
      },
      "source": [
        "df.head()"
      ],
      "id": "65826893",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>../input/cnn-data/output/27.0//173.png</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>../input/cnn-data/output/27.0//248.png</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>../input/cnn-data/output/27.0//94.png</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>../input/cnn-data/output/27.0//236.png</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>../input/cnn-data/output/27.0//340.png</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 filename category\n",
              "0  ../input/cnn-data/output/27.0//173.png     27.0\n",
              "1  ../input/cnn-data/output/27.0//248.png     27.0\n",
              "2   ../input/cnn-data/output/27.0//94.png     27.0\n",
              "3  ../input/cnn-data/output/27.0//236.png     27.0\n",
              "4  ../input/cnn-data/output/27.0//340.png     27.0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:20.102323Z",
          "iopub.status.busy": "2021-09-28T13:37:20.101546Z",
          "iopub.status.idle": "2021-09-28T13:37:20.104202Z",
          "shell.execute_reply": "2021-09-28T13:37:20.103763Z",
          "shell.execute_reply.started": "2021-09-28T08:47:06.924905Z"
        },
        "papermill": {
          "duration": 0.028476,
          "end_time": "2021-09-28T13:37:20.104319",
          "exception": false,
          "start_time": "2021-09-28T13:37:20.075843",
          "status": "completed"
        },
        "tags": [],
        "id": "947431d1"
      },
      "source": [
        "FAST_RUN = False   #Given image size is 224*224 so for ease of use for augmentation we fix standard image width and height. \n",
        "IMAGE_WIDTH=256\n",
        "IMAGE_HEIGHT=256"
      ],
      "id": "947431d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:20.152672Z",
          "iopub.status.busy": "2021-09-28T13:37:20.152033Z",
          "iopub.status.idle": "2021-09-28T13:37:20.154937Z",
          "shell.execute_reply": "2021-09-28T13:37:20.154535Z",
          "shell.execute_reply.started": "2021-09-28T08:47:06.965585Z"
        },
        "papermill": {
          "duration": 0.028581,
          "end_time": "2021-09-28T13:37:20.155060",
          "exception": false,
          "start_time": "2021-09-28T13:37:20.126479",
          "status": "completed"
        },
        "tags": [],
        "id": "f7e33d26"
      },
      "source": [
        "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
        "IMAGE_CHANNELS=1"
      ],
      "id": "f7e33d26",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:20.205426Z",
          "iopub.status.busy": "2021-09-28T13:37:20.204651Z",
          "iopub.status.idle": "2021-09-28T13:37:20.207156Z",
          "shell.execute_reply": "2021-09-28T13:37:20.206666Z",
          "shell.execute_reply.started": "2021-09-28T08:47:06.974912Z"
        },
        "papermill": {
          "duration": 0.029904,
          "end_time": "2021-09-28T13:37:20.207259",
          "exception": false,
          "start_time": "2021-09-28T13:37:20.177355",
          "status": "completed"
        },
        "tags": [],
        "id": "d117bc44"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, UpSampling2D, Concatenate,MaxPooling2D,Dropout,Flatten,Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, Callback\n",
        "\n"
      ],
      "id": "d117bc44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:20.257464Z",
          "iopub.status.busy": "2021-09-28T13:37:20.256687Z",
          "iopub.status.idle": "2021-09-28T13:37:20.258658Z",
          "shell.execute_reply": "2021-09-28T13:37:20.259078Z",
          "shell.execute_reply.started": "2021-09-28T08:47:06.983812Z"
        },
        "papermill": {
          "duration": 0.029769,
          "end_time": "2021-09-28T13:37:20.259193",
          "exception": false,
          "start_time": "2021-09-28T13:37:20.229424",
          "status": "completed"
        },
        "tags": [],
        "id": "ba72fc83"
      },
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
        "from tensorflow.keras.layers import ZeroPadding2D\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers"
      ],
      "id": "ba72fc83",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:20.306471Z",
          "iopub.status.busy": "2021-09-28T13:37:20.305708Z",
          "iopub.status.idle": "2021-09-28T13:37:20.308084Z",
          "shell.execute_reply": "2021-09-28T13:37:20.307651Z",
          "shell.execute_reply.started": "2021-09-28T08:47:06.992534Z"
        },
        "papermill": {
          "duration": 0.027244,
          "end_time": "2021-09-28T13:37:20.308194",
          "exception": false,
          "start_time": "2021-09-28T13:37:20.280950",
          "status": "completed"
        },
        "tags": [],
        "id": "ec9a4502"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "id": "ec9a4502",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:20.358786Z",
          "iopub.status.busy": "2021-09-28T13:37:20.357975Z",
          "iopub.status.idle": "2021-09-28T13:37:20.359916Z",
          "shell.execute_reply": "2021-09-28T13:37:20.360337Z",
          "shell.execute_reply.started": "2021-09-28T08:47:07.001217Z"
        },
        "papermill": {
          "duration": 0.03041,
          "end_time": "2021-09-28T13:37:20.360450",
          "exception": false,
          "start_time": "2021-09-28T13:37:20.330040",
          "status": "completed"
        },
        "tags": [],
        "id": "68939846"
      },
      "source": [
        "\n",
        "# Creating a identity block contains of conv layer , activation layer , and batch normal layer\n",
        "def identity_block(input_tensor, kernel_size, filters):\n",
        "    \n",
        "    filters1, filters2, filters3 = filters\n",
        "    \n",
        "    \n",
        "\n",
        "    x = Conv2D(filters1,1)(input_tensor)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(tf.keras.layers.LeakyReLU(alpha=0.3))(x)\n",
        "\n",
        "    x = Conv2D(filters2, kernel_size,padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(tf.keras.layers.LeakyReLU(alpha=0.3))(x)\n",
        "\n",
        "    x = Conv2D(filters3, 1)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = layers.add([x, input_tensor])\n",
        "    x = Activation(tf.keras.layers.LeakyReLU(alpha=0.3))(x)\n",
        "    return x"
      ],
      "id": "68939846",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:20.407910Z",
          "iopub.status.busy": "2021-09-28T13:37:20.407181Z",
          "iopub.status.idle": "2021-09-28T13:37:20.409132Z",
          "shell.execute_reply": "2021-09-28T13:37:20.409571Z",
          "shell.execute_reply.started": "2021-09-28T08:47:07.011330Z"
        },
        "papermill": {
          "duration": 0.027505,
          "end_time": "2021-09-28T13:37:20.409681",
          "exception": false,
          "start_time": "2021-09-28T13:37:20.382176",
          "status": "completed"
        },
        "tags": [],
        "id": "71a5ccc2"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "id": "71a5ccc2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:20.456656Z",
          "iopub.status.busy": "2021-09-28T13:37:20.455880Z",
          "iopub.status.idle": "2021-09-28T13:37:20.462702Z",
          "shell.execute_reply": "2021-09-28T13:37:20.463240Z",
          "shell.execute_reply.started": "2021-09-28T08:47:07.018440Z"
        },
        "papermill": {
          "duration": 0.03196,
          "end_time": "2021-09-28T13:37:20.463364",
          "exception": false,
          "start_time": "2021-09-28T13:37:20.431404",
          "status": "completed"
        },
        "tags": [],
        "id": "e441c824"
      },
      "source": [
        "# Creating a conv block and having resnet connection with the output of conv block\n",
        "\n",
        "def conv_block(input_tensor, kernel_size, filters, strides=(2, 2)):\n",
        "    \n",
        "    \n",
        "    filters1, filters2, filters3 = filters\n",
        "\n",
        "    x = Conv2D(filters1, 1, strides=strides)(input_tensor)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(tf.keras.layers.LeakyReLU(alpha=0.3))(x)\n",
        "\n",
        "    x = Conv2D(filters2, kernel_size, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(tf.keras.layers.LeakyReLU(alpha=0.3))(x)\n",
        "\n",
        "    x = Conv2D(filters3,1)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    shortcut = Conv2D(filters3,  1, strides=strides)(input_tensor)\n",
        "    shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "    x = layers.add([x, shortcut])\n",
        "    x = Activation(tf.keras.layers.LeakyReLU(alpha=0.3))(x)\n",
        "    return x"
      ],
      "id": "e441c824",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:20.518108Z",
          "iopub.status.busy": "2021-09-28T13:37:20.509819Z",
          "iopub.status.idle": "2021-09-28T13:37:23.570565Z",
          "shell.execute_reply": "2021-09-28T13:37:23.570101Z",
          "shell.execute_reply.started": "2021-09-28T08:47:07.029057Z"
        },
        "papermill": {
          "duration": 3.085559,
          "end_time": "2021-09-28T13:37:23.570690",
          "exception": false,
          "start_time": "2021-09-28T13:37:20.485131",
          "status": "completed"
        },
        "tags": [],
        "id": "59ec5149",
        "outputId": "d965e0a5-a53f-4976-be05-6559c2be27af"
      },
      "source": [
        "inputs = Input(shape=(224,224,1))\n",
        "\n",
        "x = ZeroPadding2D((3, 3))(inputs)\n",
        "x = Conv2D(64, (7, 7), strides=(2, 2))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(tf.keras.layers.LeakyReLU(alpha=0.3))(x)\n",
        "x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "x = conv_block(x, 3, [64, 64, 256],  strides=(1, 1))\n",
        "x = identity_block(x, 3, [64, 64, 256] )\n",
        "x = identity_block(x, 3, [64, 64, 256])\n",
        "\n",
        "x = conv_block(x, 3, [128, 128, 512])\n",
        "x = identity_block(x, 3, [128, 128, 512])\n",
        "x = identity_block(x, 3, [128, 128, 512])\n",
        "x = identity_block(x, 3, [128, 128, 512])\n",
        "\n",
        "x = conv_block(x, 3, [256, 256, 1024])\n",
        "x = identity_block(x, 3, [256, 256, 1024])\n",
        "x = identity_block(x, 3, [256, 256, 1024])\n",
        "x = identity_block(x, 3, [256, 256, 1024])\n",
        "x = identity_block(x, 3, [256, 256, 1024])\n",
        "x = identity_block(x, 3, [256, 256, 1024])\n",
        "\n",
        "x = conv_block(x, 3, [512, 512, 2048])\n",
        "x = identity_block(x, 3, [512, 512, 2048])\n",
        "x = identity_block(x, 3, [512, 512, 2048])\n",
        "\n",
        "x = AveragePooling2D((7, 7))(x)\n",
        "\n",
        "   \n",
        "x = Flatten()(x)\n",
        "x = Dense(28, activation='softmax')(x)\n",
        "\n",
        "resmodel = Model(inputs, x)\n",
        "resmodel.compile(loss='categorical_crossentropy', optimizer='Adamax', metrics=['accuracy'])"
      ],
      "id": "59ec5149",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-09-28 13:37:20.544287: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-09-28 13:37:20.547341: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-09-28 13:37:20.588135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-28 13:37:20.588750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2021-09-28 13:37:20.588813: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-09-28 13:37:20.616619: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-09-28 13:37:20.616705: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-09-28 13:37:20.632149: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-09-28 13:37:20.640461: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-09-28 13:37:20.666137: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-09-28 13:37:20.674133: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-09-28 13:37:20.676909: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-09-28 13:37:20.677103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-28 13:37:20.677805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-28 13:37:20.679350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-09-28 13:37:20.679757: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-09-28 13:37:20.679969: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-09-28 13:37:20.680140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-28 13:37:20.680705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2021-09-28 13:37:20.680742: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-09-28 13:37:20.680767: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-09-28 13:37:20.680784: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-09-28 13:37:20.680832: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-09-28 13:37:20.680850: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-09-28 13:37:20.680868: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-09-28 13:37:20.680887: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-09-28 13:37:20.680905: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-09-28 13:37:20.680982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-28 13:37:20.681618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-28 13:37:20.682156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-09-28 13:37:20.684090: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-09-28 13:37:22.181596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-09-28 13:37:22.181646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
            "2021-09-28 13:37:22.181656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
            "2021-09-28 13:37:22.184036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-28 13:37:22.184785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-28 13:37:22.185501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-28 13:37:22.186075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14957 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:23.619949Z",
          "iopub.status.busy": "2021-09-28T13:37:23.619178Z",
          "iopub.status.idle": "2021-09-28T13:37:23.700377Z",
          "shell.execute_reply": "2021-09-28T13:37:23.701515Z",
          "shell.execute_reply.started": "2021-09-28T08:47:07.960757Z"
        },
        "papermill": {
          "duration": 0.112351,
          "end_time": "2021-09-28T13:37:23.705920",
          "exception": false,
          "start_time": "2021-09-28T13:37:23.593569",
          "status": "completed"
        },
        "tags": [],
        "id": "c1d7b0d5",
        "outputId": "fd558a7e-9baa-4ff4-b882-3e0c16539881"
      },
      "source": [
        "resmodel.summary()#Summary of the model"
      ],
      "id": "c1d7b0d5",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2D)  (None, 230, 230, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 112, 112, 64) 3200        zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 112, 112, 64) 256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 112, 112, 64) 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 55, 55, 64)   0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 55, 55, 64)   4160        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 55, 55, 64)   256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 55, 55, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 55, 55, 64)   36928       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 55, 55, 64)   256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 55, 55, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 55, 55, 256)  16640       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 55, 55, 256)  16640       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 55, 55, 256)  1024        conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 55, 55, 256)  1024        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 55, 55, 256)  0           batch_normalization_3[0][0]      \n",
            "                                                                 batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 55, 55, 256)  0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 55, 55, 64)   16448       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 55, 55, 64)   256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 55, 55, 64)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 55, 55, 64)   36928       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 55, 55, 64)   256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 55, 55, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 55, 55, 256)  16640       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 55, 55, 256)  1024        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 55, 55, 256)  0           batch_normalization_7[0][0]      \n",
            "                                                                 activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 55, 55, 256)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 55, 55, 64)   16448       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 55, 55, 64)   256         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 55, 55, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 55, 55, 64)   36928       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 55, 55, 64)   256         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 55, 55, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 55, 55, 256)  16640       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 55, 55, 256)  1024        conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 55, 55, 256)  0           batch_normalization_10[0][0]     \n",
            "                                                                 activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 55, 55, 256)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 28, 28, 128)  32896       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 28, 28, 128)  512         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 28, 28, 128)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 28, 28, 128)  147584      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 28, 28, 128)  512         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 28, 28, 128)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 28, 28, 512)  66048       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 28, 28, 512)  131584      activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 28, 28, 512)  2048        conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 28, 28, 512)  2048        conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 28, 28, 512)  0           batch_normalization_13[0][0]     \n",
            "                                                                 batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 28, 28, 512)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 28, 28, 128)  65664       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 28, 28, 128)  512         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 28, 28, 128)  0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 28, 28, 128)  147584      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 28, 28, 128)  512         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 28, 28, 128)  0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 28, 28, 512)  66048       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 28, 28, 512)  2048        conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 28, 28, 512)  0           batch_normalization_17[0][0]     \n",
            "                                                                 activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 28, 28, 512)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 28, 28, 128)  65664       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 28, 28, 128)  512         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 28, 28, 128)  0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 28, 28, 128)  147584      activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 28, 28, 128)  512         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 28, 28, 128)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 28, 28, 512)  66048       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 28, 28, 512)  2048        conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 28, 28, 512)  0           batch_normalization_20[0][0]     \n",
            "                                                                 activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 28, 28, 512)  0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 28, 28, 128)  65664       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 28, 28, 128)  512         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 28, 28, 128)  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 28, 28, 128)  147584      activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 28, 28, 128)  512         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 28, 28, 128)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 28, 28, 512)  66048       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 28, 28, 512)  2048        conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 28, 28, 512)  0           batch_normalization_23[0][0]     \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 28, 28, 512)  0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 14, 14, 256)  131328      activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 14, 14, 256)  1024        conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 14, 14, 256)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 14, 14, 256)  590080      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 14, 14, 256)  1024        conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 14, 14, 256)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 14, 14, 1024) 263168      activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 14, 14, 1024) 525312      activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 14, 14, 1024) 4096        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 14, 14, 1024) 4096        conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 14, 14, 1024) 0           batch_normalization_26[0][0]     \n",
            "                                                                 batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 14, 14, 1024) 0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 14, 14, 256)  262400      activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 14, 14, 256)  1024        conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 14, 14, 256)  0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 14, 14, 256)  590080      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 14, 14, 256)  1024        conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 14, 14, 256)  0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 14, 14, 1024) 263168      activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 14, 14, 1024) 4096        conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 14, 14, 1024) 0           batch_normalization_30[0][0]     \n",
            "                                                                 activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 14, 14, 1024) 0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 14, 14, 256)  262400      activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 14, 14, 256)  1024        conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 14, 14, 256)  0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 14, 14, 256)  590080      activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 14, 14, 256)  1024        conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 14, 14, 256)  0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 14, 14, 1024) 263168      activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 14, 14, 1024) 4096        conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 14, 14, 1024) 0           batch_normalization_33[0][0]     \n",
            "                                                                 activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 14, 14, 1024) 0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 14, 14, 256)  262400      activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 14, 14, 256)  1024        conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 14, 14, 256)  0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 14, 14, 256)  590080      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 14, 14, 256)  1024        conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 14, 14, 256)  0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 14, 14, 1024) 263168      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 14, 14, 1024) 4096        conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 14, 14, 1024) 0           batch_normalization_36[0][0]     \n",
            "                                                                 activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 14, 14, 1024) 0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 14, 14, 256)  262400      activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 14, 14, 256)  1024        conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 14, 14, 256)  0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 14, 14, 256)  590080      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 14, 14, 256)  1024        conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 14, 14, 256)  0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 14, 14, 1024) 263168      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 14, 14, 1024) 4096        conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 14, 14, 1024) 0           batch_normalization_39[0][0]     \n",
            "                                                                 activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 14, 14, 1024) 0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 14, 14, 256)  262400      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 14, 14, 256)  1024        conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 14, 14, 256)  0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 14, 14, 256)  590080      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 14, 14, 256)  1024        conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 14, 14, 256)  0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 14, 14, 1024) 263168      activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 14, 14, 1024) 4096        conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 14, 14, 1024) 0           batch_normalization_42[0][0]     \n",
            "                                                                 activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 14, 14, 1024) 0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 7, 7, 512)    524800      activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 7, 7, 512)    2048        conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 7, 7, 512)    0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 7, 7, 512)    2359808     activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 7, 7, 512)    2048        conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 7, 7, 512)    0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 7, 7, 2048)   1050624     activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 7, 7, 2048)   2099200     activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 7, 7, 2048)   8192        conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 7, 7, 2048)   8192        conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 7, 7, 2048)   0           batch_normalization_45[0][0]     \n",
            "                                                                 batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 7, 7, 2048)   0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 7, 7, 512)    1049088     activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 7, 7, 512)    2048        conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 7, 7, 512)    0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 7, 7, 512)    2359808     activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 7, 7, 512)    2048        conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 7, 7, 512)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 7, 7, 2048)   1050624     activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 7, 7, 2048)   8192        conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 7, 7, 2048)   0           batch_normalization_49[0][0]     \n",
            "                                                                 activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 7, 7, 2048)   0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 7, 7, 512)    1049088     activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 7, 7, 512)    2048        conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 7, 7, 512)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 7, 7, 512)    2359808     activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 7, 7, 512)    2048        conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 7, 7, 512)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 7, 7, 2048)   1050624     activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 7, 7, 2048)   8192        conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 7, 7, 2048)   0           batch_normalization_52[0][0]     \n",
            "                                                                 activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 7, 7, 2048)   0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 1, 1, 2048)   0           activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 2048)         0           average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 28)           57372       flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 23,638,812\n",
            "Trainable params: 23,585,692\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:23.760593Z",
          "iopub.status.busy": "2021-09-28T13:37:23.759828Z",
          "iopub.status.idle": "2021-09-28T13:37:24.199343Z",
          "shell.execute_reply": "2021-09-28T13:37:24.200080Z",
          "shell.execute_reply.started": "2021-09-28T08:47:08.052991Z"
        },
        "papermill": {
          "duration": 0.47002,
          "end_time": "2021-09-28T13:37:24.200229",
          "exception": false,
          "start_time": "2021-09-28T13:37:23.730209",
          "status": "completed"
        },
        "tags": [],
        "id": "42203866",
        "outputId": "75b7c44d-7f0f-4ebb-f135-2b1286ccc689"
      },
      "source": [
        "from sklearn.model_selection import train_test_split #Simple plot of No of Images vs Class Labels\n",
        "df = df.sample(frac = 1)\n",
        "\n",
        "plt.figure(figsize = (20,20))\n",
        "df['category'].value_counts().plot.bar()"
      ],
      "id": "42203866",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1440x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:24.254612Z",
          "iopub.status.busy": "2021-09-28T13:37:24.253786Z",
          "iopub.status.idle": "2021-09-28T13:37:24.256171Z",
          "shell.execute_reply": "2021-09-28T13:37:24.255766Z",
          "shell.execute_reply.started": "2021-09-28T08:47:08.457505Z"
        },
        "papermill": {
          "duration": 0.031308,
          "end_time": "2021-09-28T13:37:24.256283",
          "exception": false,
          "start_time": "2021-09-28T13:37:24.224975",
          "status": "completed"
        },
        "tags": [],
        "id": "822bdc12"
      },
      "source": [
        "total_train = df.shape[0]  \n",
        "total_validate = df.shape[0]\n",
        "batch_size = 64"
      ],
      "id": "822bdc12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHGaCf5566D2"
      },
      "source": [
        "## Image Data Generator Concept keras :https://towardsdatascience.com/keras-data-generators-and-how-to-use-them-b69129ed779c"
      ],
      "id": "UHGaCf5566D2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:24.320928Z",
          "iopub.status.busy": "2021-09-28T13:37:24.320347Z",
          "iopub.status.idle": "2021-09-28T13:37:25.649573Z",
          "shell.execute_reply": "2021-09-28T13:37:25.650260Z",
          "shell.execute_reply.started": "2021-09-28T08:47:08.465590Z"
        },
        "papermill": {
          "duration": 1.369326,
          "end_time": "2021-09-28T13:37:25.650418",
          "exception": false,
          "start_time": "2021-09-28T13:37:24.281092",
          "status": "completed"
        },
        "tags": [],
        "id": "28248d18",
        "outputId": "6ee2f86a-73d4-45d7-cf89-d68d371683e6"
      },
      "source": [
        "# Image data generators \n",
        "\n",
        "t_datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    rescale=1./255,\n",
        "    shear_range=0.1,                 #Adding augmentation techniques like rotation_range,rescale,shear_range etc\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        ")\n",
        "\n",
        "t_generator = t_datagen.flow_from_dataframe(\n",
        "    df, \n",
        "    color_mode=\"grayscale\",\n",
        "    x_col='filename',\n",
        "    y_col='category',\n",
        "    target_size=(224,224),\n",
        "    class_mode='categorical',\n",
        "    batch_size=batch_size\n",
        ")\n"
      ],
      "id": "28248d18",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3602 validated image filenames belonging to 28 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzFhtP2I_ie-"
      },
      "source": [
        "Refernce for augmentation techniques : https://www.analyticsvidhya.com/blog/2020/08/image-augmentation-on-the-fly-using-keras-imagedatagenerator/"
      ],
      "id": "EzFhtP2I_ie-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:25.707150Z",
          "iopub.status.busy": "2021-09-28T13:37:25.706494Z",
          "iopub.status.idle": "2021-09-28T13:37:25.708967Z",
          "shell.execute_reply": "2021-09-28T13:37:25.709387Z",
          "shell.execute_reply.started": "2021-09-28T08:47:09.747372Z"
        },
        "papermill": {
          "duration": 0.034074,
          "end_time": "2021-09-28T13:37:25.709511",
          "exception": false,
          "start_time": "2021-09-28T13:37:25.675437",
          "status": "completed"
        },
        "tags": [],
        "id": "9e8d385f",
        "outputId": "c9a6db22-ab6c-4170-d0e7-568e23187192"
      },
      "source": [
        "t_generator.class_indices   #Putting class indices in Training Generator"
      ],
      "id": "9e8d385f",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'0.0': 0,\n",
              " '1.0': 1,\n",
              " '11.0': 2,\n",
              " '12.0': 3,\n",
              " '13.0': 4,\n",
              " '14.0': 5,\n",
              " '15.0': 6,\n",
              " '16.0': 7,\n",
              " '17.0': 8,\n",
              " '18.0': 9,\n",
              " '19.0': 10,\n",
              " '2.0': 11,\n",
              " '20.0': 12,\n",
              " '21.0': 13,\n",
              " '22.0': 14,\n",
              " '23.0': 15,\n",
              " '24.0': 16,\n",
              " '25.0': 17,\n",
              " '26.0': 18,\n",
              " '27.0': 19,\n",
              " '28.0': 20,\n",
              " '3.0': 21,\n",
              " '4.0': 22,\n",
              " '5.0': 23,\n",
              " '6.0': 24,\n",
              " '7.0': 25,\n",
              " '8.0': 26,\n",
              " '9.0': 27}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:25.765124Z",
          "iopub.status.busy": "2021-09-28T13:37:25.764476Z",
          "iopub.status.idle": "2021-09-28T13:37:25.767253Z",
          "shell.execute_reply": "2021-09-28T13:37:25.767693Z",
          "shell.execute_reply.started": "2021-09-28T08:47:09.758036Z"
        },
        "papermill": {
          "duration": 0.032972,
          "end_time": "2021-09-28T13:37:25.767810",
          "exception": false,
          "start_time": "2021-09-28T13:37:25.734838",
          "status": "completed"
        },
        "tags": [],
        "id": "6a583153",
        "outputId": "83b30289-48cd-465f-910b-180d839b749c"
      },
      "source": [
        "len(d)"
      ],
      "id": "6a583153",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:25.822494Z",
          "iopub.status.busy": "2021-09-28T13:37:25.821705Z",
          "iopub.status.idle": "2021-09-28T13:37:25.823708Z",
          "shell.execute_reply": "2021-09-28T13:37:25.824146Z",
          "shell.execute_reply.started": "2021-09-28T08:47:09.768121Z"
        },
        "papermill": {
          "duration": 0.030887,
          "end_time": "2021-09-28T13:37:25.824276",
          "exception": false,
          "start_time": "2021-09-28T13:37:25.793389",
          "status": "completed"
        },
        "tags": [],
        "id": "cd5f5ba0"
      },
      "source": [
        "d = t_generator.class_indices\n"
      ],
      "id": "cd5f5ba0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:25.877700Z",
          "iopub.status.busy": "2021-09-28T13:37:25.876902Z",
          "iopub.status.idle": "2021-09-28T13:37:25.881252Z",
          "shell.execute_reply": "2021-09-28T13:37:25.881640Z",
          "shell.execute_reply.started": "2021-09-28T08:47:09.776659Z"
        },
        "papermill": {
          "duration": 0.03216,
          "end_time": "2021-09-28T13:37:25.881756",
          "exception": false,
          "start_time": "2021-09-28T13:37:25.849596",
          "status": "completed"
        },
        "tags": [],
        "id": "b79c1081"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau  # Saving only best models and weights.\n",
        "red = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, verbose=1, min_lr=1e-3)\n",
        "checkpoint = ModelCheckpoint('model.h5', verbose=1, save_best_only=True)"
      ],
      "id": "b79c1081",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T13:37:25.938556Z",
          "iopub.status.busy": "2021-09-28T13:37:25.935933Z",
          "iopub.status.idle": "2021-09-28T18:42:18.876678Z",
          "shell.execute_reply": "2021-09-28T18:42:18.875836Z",
          "shell.execute_reply.started": "2021-09-28T08:47:09.784516Z"
        },
        "papermill": {
          "duration": 18292.969431,
          "end_time": "2021-09-28T18:42:18.876819",
          "exception": false,
          "start_time": "2021-09-28T13:37:25.907388",
          "status": "completed"
        },
        "tags": [],
        "id": "a1c8d8b6",
        "outputId": "8a725ffe-397c-4bc0-b6cc-d4830b44c7aa"
      },
      "source": [
        "# Model training for 600 epochs\n",
        "\n",
        "epochs=600\n",
        "modelhistory = resmodel.fit(\n",
        "    t_generator, \n",
        "    epochs=epochs,\n",
        "    \n",
        "    steps_per_epoch=len(df)//batch_size,callbacks=[red, checkpoint])\n",
        "    \n"
      ],
      "id": "a1c8d8b6",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-09-28 13:37:27.203087: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-09-28 13:37:27.207455: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000175000 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-09-28 13:37:31.631105: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-09-28 13:37:32.494971: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-09-28 13:37:32.798300: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "56/56 [==============================] - 67s 992ms/step - loss: 4.7429 - accuracy: 0.0863\n",
            "Epoch 2/600\n",
            "56/56 [==============================] - 29s 507ms/step - loss: 3.1508 - accuracy: 0.1026\n",
            "Epoch 3/600\n",
            "56/56 [==============================] - 29s 509ms/step - loss: 3.0960 - accuracy: 0.1073\n",
            "Epoch 4/600\n",
            "56/56 [==============================] - 29s 508ms/step - loss: 3.0783 - accuracy: 0.1030\n",
            "Epoch 5/600\n",
            "56/56 [==============================] - 28s 503ms/step - loss: 3.0482 - accuracy: 0.1180\n",
            "Epoch 6/600\n",
            "56/56 [==============================] - 29s 507ms/step - loss: 3.0256 - accuracy: 0.1213\n",
            "Epoch 7/600\n",
            "56/56 [==============================] - 28s 495ms/step - loss: 3.0484 - accuracy: 0.1049\n",
            "Epoch 8/600\n",
            "56/56 [==============================] - 29s 509ms/step - loss: 3.0278 - accuracy: 0.1239\n",
            "Epoch 9/600\n",
            "56/56 [==============================] - 29s 509ms/step - loss: 3.0111 - accuracy: 0.1245\n",
            "Epoch 10/600\n",
            "56/56 [==============================] - 28s 505ms/step - loss: 2.9886 - accuracy: 0.1051\n",
            "Epoch 11/600\n",
            "56/56 [==============================] - 29s 506ms/step - loss: 2.9923 - accuracy: 0.1239\n",
            "Epoch 12/600\n",
            "56/56 [==============================] - 29s 508ms/step - loss: 2.9935 - accuracy: 0.1230\n",
            "Epoch 13/600\n",
            "56/56 [==============================] - 29s 506ms/step - loss: 2.9827 - accuracy: 0.1227\n",
            "Epoch 14/600\n",
            "56/56 [==============================] - 28s 501ms/step - loss: 2.9971 - accuracy: 0.1271\n",
            "Epoch 15/600\n",
            "56/56 [==============================] - 28s 496ms/step - loss: 2.9535 - accuracy: 0.1396\n",
            "Epoch 16/600\n",
            "56/56 [==============================] - 29s 509ms/step - loss: 2.9521 - accuracy: 0.1223\n",
            "Epoch 17/600\n",
            "56/56 [==============================] - 28s 505ms/step - loss: 2.9810 - accuracy: 0.1293\n",
            "Epoch 18/600\n",
            "56/56 [==============================] - 29s 504ms/step - loss: 2.9625 - accuracy: 0.1235\n",
            "Epoch 19/600\n",
            "56/56 [==============================] - 29s 511ms/step - loss: 2.9892 - accuracy: 0.1229\n",
            "Epoch 20/600\n",
            "56/56 [==============================] - 28s 504ms/step - loss: 2.9318 - accuracy: 0.1506\n",
            "Epoch 21/600\n",
            "56/56 [==============================] - 29s 511ms/step - loss: 2.9309 - accuracy: 0.1449\n",
            "Epoch 22/600\n",
            "56/56 [==============================] - 28s 501ms/step - loss: 2.9117 - accuracy: 0.1405\n",
            "Epoch 23/600\n",
            "56/56 [==============================] - 29s 511ms/step - loss: 2.9208 - accuracy: 0.1405\n",
            "Epoch 24/600\n",
            "56/56 [==============================] - 28s 502ms/step - loss: 2.9247 - accuracy: 0.1337\n",
            "Epoch 25/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 2.9086 - accuracy: 0.1355\n",
            "Epoch 26/600\n",
            "56/56 [==============================] - 30s 524ms/step - loss: 2.8609 - accuracy: 0.1500\n",
            "Epoch 27/600\n",
            "56/56 [==============================] - 29s 521ms/step - loss: 2.8945 - accuracy: 0.1540\n",
            "Epoch 28/600\n",
            "56/56 [==============================] - 29s 511ms/step - loss: 2.8819 - accuracy: 0.1538\n",
            "Epoch 29/600\n",
            "56/56 [==============================] - 29s 511ms/step - loss: 2.8720 - accuracy: 0.1300\n",
            "Epoch 30/600\n",
            "56/56 [==============================] - 29s 513ms/step - loss: 2.8500 - accuracy: 0.1453\n",
            "Epoch 31/600\n",
            "56/56 [==============================] - 28s 501ms/step - loss: 2.8438 - accuracy: 0.1584\n",
            "Epoch 32/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 2.8582 - accuracy: 0.1484\n",
            "Epoch 33/600\n",
            "56/56 [==============================] - 29s 510ms/step - loss: 2.8186 - accuracy: 0.1625\n",
            "Epoch 34/600\n",
            "56/56 [==============================] - 29s 509ms/step - loss: 2.8323 - accuracy: 0.1584\n",
            "Epoch 35/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 2.8274 - accuracy: 0.1680\n",
            "Epoch 36/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 2.7978 - accuracy: 0.1689\n",
            "Epoch 37/600\n",
            "56/56 [==============================] - 29s 518ms/step - loss: 2.7770 - accuracy: 0.1697\n",
            "Epoch 38/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 2.7717 - accuracy: 0.1739\n",
            "Epoch 39/600\n",
            "56/56 [==============================] - 29s 508ms/step - loss: 2.7720 - accuracy: 0.1728\n",
            "Epoch 40/600\n",
            "56/56 [==============================] - 29s 501ms/step - loss: 2.7399 - accuracy: 0.1820\n",
            "Epoch 41/600\n",
            "56/56 [==============================] - 29s 511ms/step - loss: 2.7428 - accuracy: 0.1792\n",
            "Epoch 42/600\n",
            "56/56 [==============================] - 29s 510ms/step - loss: 2.7420 - accuracy: 0.1929\n",
            "Epoch 43/600\n",
            "56/56 [==============================] - 29s 517ms/step - loss: 2.6830 - accuracy: 0.1899\n",
            "Epoch 44/600\n",
            "56/56 [==============================] - 29s 519ms/step - loss: 2.6915 - accuracy: 0.1853\n",
            "Epoch 45/600\n",
            "56/56 [==============================] - 29s 511ms/step - loss: 2.6469 - accuracy: 0.2076\n",
            "Epoch 46/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 2.6584 - accuracy: 0.2080\n",
            "Epoch 47/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 2.6061 - accuracy: 0.2162\n",
            "Epoch 48/600\n",
            "56/56 [==============================] - 29s 511ms/step - loss: 2.6320 - accuracy: 0.2029\n",
            "Epoch 49/600\n",
            "56/56 [==============================] - 29s 517ms/step - loss: 2.6524 - accuracy: 0.2086\n",
            "Epoch 50/600\n",
            "56/56 [==============================] - 29s 513ms/step - loss: 2.6147 - accuracy: 0.2043\n",
            "Epoch 51/600\n",
            "56/56 [==============================] - 28s 503ms/step - loss: 2.5765 - accuracy: 0.2357\n",
            "Epoch 52/600\n",
            "56/56 [==============================] - 29s 516ms/step - loss: 2.5499 - accuracy: 0.2340\n",
            "Epoch 53/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 2.4905 - accuracy: 0.2494\n",
            "Epoch 54/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 2.5282 - accuracy: 0.2446\n",
            "Epoch 55/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 2.4509 - accuracy: 0.2568\n",
            "Epoch 56/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 2.4911 - accuracy: 0.2407\n",
            "Epoch 57/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 2.4899 - accuracy: 0.2466\n",
            "Epoch 58/600\n",
            "56/56 [==============================] - 29s 518ms/step - loss: 2.4198 - accuracy: 0.2572\n",
            "Epoch 59/600\n",
            "56/56 [==============================] - 29s 513ms/step - loss: 2.3684 - accuracy: 0.2899\n",
            "Epoch 60/600\n",
            "56/56 [==============================] - 29s 503ms/step - loss: 2.3881 - accuracy: 0.2798\n",
            "Epoch 61/600\n",
            "56/56 [==============================] - 29s 507ms/step - loss: 2.3578 - accuracy: 0.2927\n",
            "Epoch 62/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 2.3298 - accuracy: 0.2879\n",
            "Epoch 63/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 2.3320 - accuracy: 0.3129\n",
            "Epoch 64/600\n",
            "56/56 [==============================] - 29s 517ms/step - loss: 2.2699 - accuracy: 0.3205\n",
            "Epoch 65/600\n",
            "56/56 [==============================] - 29s 516ms/step - loss: 2.3022 - accuracy: 0.3028\n",
            "Epoch 66/600\n",
            "56/56 [==============================] - 29s 523ms/step - loss: 2.3013 - accuracy: 0.2902\n",
            "Epoch 67/600\n",
            "56/56 [==============================] - 29s 519ms/step - loss: 2.2390 - accuracy: 0.3278\n",
            "Epoch 68/600\n",
            "56/56 [==============================] - 30s 533ms/step - loss: 2.2783 - accuracy: 0.3138\n",
            "Epoch 69/600\n",
            "56/56 [==============================] - 30s 535ms/step - loss: 2.2278 - accuracy: 0.3250\n",
            "Epoch 70/600\n",
            "56/56 [==============================] - 30s 537ms/step - loss: 2.2410 - accuracy: 0.3170\n",
            "Epoch 71/600\n",
            "56/56 [==============================] - 30s 534ms/step - loss: 2.1582 - accuracy: 0.3373\n",
            "Epoch 72/600\n",
            "56/56 [==============================] - 30s 530ms/step - loss: 2.1564 - accuracy: 0.3348\n",
            "Epoch 73/600\n",
            "56/56 [==============================] - 30s 533ms/step - loss: 2.1525 - accuracy: 0.3508\n",
            "Epoch 74/600\n",
            "56/56 [==============================] - 30s 533ms/step - loss: 2.1351 - accuracy: 0.3519\n",
            "Epoch 75/600\n",
            "56/56 [==============================] - 29s 519ms/step - loss: 2.1273 - accuracy: 0.3487\n",
            "Epoch 76/600\n",
            "56/56 [==============================] - 30s 533ms/step - loss: 2.1364 - accuracy: 0.3514\n",
            "Epoch 77/600\n",
            "56/56 [==============================] - 30s 537ms/step - loss: 2.1067 - accuracy: 0.3606\n",
            "Epoch 78/600\n",
            "56/56 [==============================] - 30s 539ms/step - loss: 2.0753 - accuracy: 0.3642\n",
            "Epoch 79/600\n",
            "56/56 [==============================] - 30s 532ms/step - loss: 2.1495 - accuracy: 0.3470\n",
            "Epoch 80/600\n",
            "56/56 [==============================] - 29s 523ms/step - loss: 2.0563 - accuracy: 0.3622\n",
            "Epoch 81/600\n",
            "56/56 [==============================] - 29s 518ms/step - loss: 2.0519 - accuracy: 0.3789\n",
            "Epoch 82/600\n",
            "56/56 [==============================] - 30s 535ms/step - loss: 2.0198 - accuracy: 0.3828\n",
            "Epoch 83/600\n",
            "56/56 [==============================] - 30s 527ms/step - loss: 2.0407 - accuracy: 0.3729\n",
            "Epoch 84/600\n",
            "56/56 [==============================] - 30s 525ms/step - loss: 1.9716 - accuracy: 0.3903\n",
            "Epoch 85/600\n",
            "56/56 [==============================] - 29s 523ms/step - loss: 2.0095 - accuracy: 0.3852\n",
            "Epoch 86/600\n",
            "56/56 [==============================] - 30s 526ms/step - loss: 1.9796 - accuracy: 0.3935\n",
            "Epoch 87/600\n",
            "56/56 [==============================] - 29s 507ms/step - loss: 1.9626 - accuracy: 0.4024\n",
            "Epoch 88/600\n",
            "56/56 [==============================] - 30s 519ms/step - loss: 1.9891 - accuracy: 0.3913\n",
            "Epoch 89/600\n",
            "56/56 [==============================] - 31s 544ms/step - loss: 1.9538 - accuracy: 0.3922\n",
            "Epoch 90/600\n",
            "56/56 [==============================] - 30s 535ms/step - loss: 1.9335 - accuracy: 0.4047\n",
            "Epoch 91/600\n",
            "56/56 [==============================] - 30s 531ms/step - loss: 1.9364 - accuracy: 0.4116\n",
            "Epoch 92/600\n",
            "56/56 [==============================] - 30s 529ms/step - loss: 1.9399 - accuracy: 0.4017\n",
            "Epoch 93/600\n",
            "56/56 [==============================] - 30s 536ms/step - loss: 1.9537 - accuracy: 0.3879\n",
            "Epoch 94/600\n",
            "56/56 [==============================] - 30s 536ms/step - loss: 1.8860 - accuracy: 0.4155\n",
            "Epoch 95/600\n",
            "56/56 [==============================] - 30s 527ms/step - loss: 1.8726 - accuracy: 0.4207\n",
            "Epoch 96/600\n",
            "56/56 [==============================] - 30s 527ms/step - loss: 1.8832 - accuracy: 0.4135\n",
            "Epoch 97/600\n",
            "56/56 [==============================] - 30s 523ms/step - loss: 1.9518 - accuracy: 0.4076\n",
            "Epoch 98/600\n",
            "56/56 [==============================] - 29s 518ms/step - loss: 1.8753 - accuracy: 0.4275\n",
            "Epoch 99/600\n",
            "56/56 [==============================] - 29s 518ms/step - loss: 1.8700 - accuracy: 0.4210\n",
            "Epoch 100/600\n",
            "56/56 [==============================] - 30s 532ms/step - loss: 1.8618 - accuracy: 0.4212\n",
            "Epoch 101/600\n",
            "56/56 [==============================] - 30s 532ms/step - loss: 1.8746 - accuracy: 0.4056\n",
            "Epoch 102/600\n",
            "56/56 [==============================] - 30s 536ms/step - loss: 1.8925 - accuracy: 0.4121\n",
            "Epoch 103/600\n",
            "56/56 [==============================] - 30s 532ms/step - loss: 1.8420 - accuracy: 0.4191\n",
            "Epoch 104/600\n",
            "56/56 [==============================] - 30s 531ms/step - loss: 1.8679 - accuracy: 0.4157\n",
            "Epoch 105/600\n",
            "56/56 [==============================] - 30s 537ms/step - loss: 1.8376 - accuracy: 0.4286\n",
            "Epoch 106/600\n",
            "56/56 [==============================] - 30s 538ms/step - loss: 1.8253 - accuracy: 0.4367\n",
            "Epoch 107/600\n",
            "56/56 [==============================] - 30s 529ms/step - loss: 1.8173 - accuracy: 0.4412\n",
            "Epoch 108/600\n",
            "56/56 [==============================] - 30s 538ms/step - loss: 1.7821 - accuracy: 0.4420\n",
            "Epoch 109/600\n",
            "56/56 [==============================] - 29s 509ms/step - loss: 1.7867 - accuracy: 0.4464\n",
            "Epoch 110/600\n",
            "56/56 [==============================] - 30s 533ms/step - loss: 1.8040 - accuracy: 0.4438\n",
            "Epoch 111/600\n",
            "56/56 [==============================] - 30s 533ms/step - loss: 1.8227 - accuracy: 0.4191\n",
            "Epoch 112/600\n",
            "56/56 [==============================] - 30s 533ms/step - loss: 1.7879 - accuracy: 0.4453\n",
            "Epoch 113/600\n",
            "56/56 [==============================] - 30s 539ms/step - loss: 1.8064 - accuracy: 0.4396\n",
            "Epoch 114/600\n",
            "56/56 [==============================] - 30s 533ms/step - loss: 1.7742 - accuracy: 0.4402\n",
            "Epoch 115/600\n",
            "56/56 [==============================] - 30s 533ms/step - loss: 1.7693 - accuracy: 0.4440\n",
            "Epoch 116/600\n",
            "56/56 [==============================] - 30s 538ms/step - loss: 1.7614 - accuracy: 0.4489\n",
            "Epoch 117/600\n",
            "56/56 [==============================] - 30s 540ms/step - loss: 1.7889 - accuracy: 0.4346\n",
            "Epoch 118/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 1.7594 - accuracy: 0.4590\n",
            "Epoch 119/600\n",
            "56/56 [==============================] - 30s 524ms/step - loss: 1.7511 - accuracy: 0.4668\n",
            "Epoch 120/600\n",
            "56/56 [==============================] - 30s 529ms/step - loss: 1.7622 - accuracy: 0.4559\n",
            "Epoch 121/600\n",
            "56/56 [==============================] - 30s 538ms/step - loss: 1.7908 - accuracy: 0.4364\n",
            "Epoch 122/600\n",
            "56/56 [==============================] - 30s 532ms/step - loss: 1.7750 - accuracy: 0.4521\n",
            "Epoch 123/600\n",
            "56/56 [==============================] - 30s 536ms/step - loss: 1.7472 - accuracy: 0.4552\n",
            "Epoch 124/600\n",
            "56/56 [==============================] - 30s 538ms/step - loss: 1.7246 - accuracy: 0.4469\n",
            "Epoch 125/600\n",
            "56/56 [==============================] - 30s 541ms/step - loss: 1.6947 - accuracy: 0.4724\n",
            "Epoch 126/600\n",
            "56/56 [==============================] - 30s 540ms/step - loss: 1.7080 - accuracy: 0.4711\n",
            "Epoch 127/600\n",
            "56/56 [==============================] - 30s 539ms/step - loss: 1.7001 - accuracy: 0.4712\n",
            "Epoch 128/600\n",
            "56/56 [==============================] - 29s 520ms/step - loss: 1.7272 - accuracy: 0.4559\n",
            "Epoch 129/600\n",
            "56/56 [==============================] - 31s 539ms/step - loss: 1.6857 - accuracy: 0.4739\n",
            "Epoch 130/600\n",
            "56/56 [==============================] - 31s 543ms/step - loss: 1.6816 - accuracy: 0.4716\n",
            "Epoch 131/600\n",
            "56/56 [==============================] - 31s 544ms/step - loss: 1.6965 - accuracy: 0.4688\n",
            "Epoch 132/600\n",
            "56/56 [==============================] - 31s 543ms/step - loss: 1.6676 - accuracy: 0.4712\n",
            "Epoch 133/600\n",
            "56/56 [==============================] - 31s 543ms/step - loss: 1.6820 - accuracy: 0.4663\n",
            "Epoch 134/600\n",
            "56/56 [==============================] - 31s 547ms/step - loss: 1.6824 - accuracy: 0.4795\n",
            "Epoch 135/600\n",
            "56/56 [==============================] - 31s 543ms/step - loss: 1.6774 - accuracy: 0.4658\n",
            "Epoch 136/600\n",
            "56/56 [==============================] - 31s 551ms/step - loss: 1.7249 - accuracy: 0.4544\n",
            "Epoch 137/600\n",
            "56/56 [==============================] - 31s 544ms/step - loss: 1.6891 - accuracy: 0.4695\n",
            "Epoch 138/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 1.6831 - accuracy: 0.4647\n",
            "Epoch 139/600\n",
            "56/56 [==============================] - 31s 544ms/step - loss: 1.6234 - accuracy: 0.4919\n",
            "Epoch 140/600\n",
            "56/56 [==============================] - 30s 550ms/step - loss: 1.6468 - accuracy: 0.4795\n",
            "Epoch 141/600\n",
            "56/56 [==============================] - 30s 538ms/step - loss: 1.6446 - accuracy: 0.4729\n",
            "Epoch 142/600\n",
            "56/56 [==============================] - 31s 542ms/step - loss: 1.6278 - accuracy: 0.4923\n",
            "Epoch 143/600\n",
            "56/56 [==============================] - 30s 537ms/step - loss: 1.6232 - accuracy: 0.4828\n",
            "Epoch 144/600\n",
            "56/56 [==============================] - 30s 541ms/step - loss: 1.6529 - accuracy: 0.4874\n",
            "Epoch 145/600\n",
            "56/56 [==============================] - 30s 541ms/step - loss: 1.6687 - accuracy: 0.4725\n",
            "Epoch 146/600\n",
            "56/56 [==============================] - 30s 525ms/step - loss: 1.6160 - accuracy: 0.4784\n",
            "Epoch 147/600\n",
            "56/56 [==============================] - 30s 526ms/step - loss: 1.6454 - accuracy: 0.4737\n",
            "Epoch 148/600\n",
            "56/56 [==============================] - 31s 548ms/step - loss: 1.6025 - accuracy: 0.4921\n",
            "Epoch 149/600\n",
            "56/56 [==============================] - 30s 541ms/step - loss: 1.5970 - accuracy: 0.5017\n",
            "Epoch 150/600\n",
            "56/56 [==============================] - 31s 543ms/step - loss: 1.6169 - accuracy: 0.4986\n",
            "Epoch 151/600\n",
            "56/56 [==============================] - 31s 547ms/step - loss: 1.6378 - accuracy: 0.4916\n",
            "Epoch 152/600\n",
            "56/56 [==============================] - 31s 547ms/step - loss: 1.6083 - accuracy: 0.4901\n",
            "Epoch 153/600\n",
            "56/56 [==============================] - 31s 547ms/step - loss: 1.5900 - accuracy: 0.4852\n",
            "Epoch 154/600\n",
            "56/56 [==============================] - 31s 545ms/step - loss: 1.6043 - accuracy: 0.4850\n",
            "Epoch 155/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 1.6149 - accuracy: 0.4870\n",
            "Epoch 156/600\n",
            "56/56 [==============================] - 31s 547ms/step - loss: 1.5779 - accuracy: 0.4982\n",
            "Epoch 157/600\n",
            "56/56 [==============================] - 31s 549ms/step - loss: 1.5552 - accuracy: 0.4941\n",
            "Epoch 158/600\n",
            "56/56 [==============================] - 31s 552ms/step - loss: 1.6186 - accuracy: 0.4873\n",
            "Epoch 159/600\n",
            "56/56 [==============================] - 31s 546ms/step - loss: 1.5425 - accuracy: 0.5016\n",
            "Epoch 160/600\n",
            "56/56 [==============================] - 31s 543ms/step - loss: 1.5465 - accuracy: 0.4939\n",
            "Epoch 161/600\n",
            "56/56 [==============================] - 31s 546ms/step - loss: 1.5510 - accuracy: 0.4973\n",
            "Epoch 162/600\n",
            "56/56 [==============================] - 31s 547ms/step - loss: 1.5498 - accuracy: 0.5067\n",
            "Epoch 163/600\n",
            "56/56 [==============================] - 30s 532ms/step - loss: 1.5307 - accuracy: 0.5114\n",
            "Epoch 164/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 1.5555 - accuracy: 0.4889\n",
            "Epoch 165/600\n",
            "56/56 [==============================] - 31s 543ms/step - loss: 1.5388 - accuracy: 0.5099\n",
            "Epoch 166/600\n",
            "56/56 [==============================] - 31s 548ms/step - loss: 1.5459 - accuracy: 0.5061\n",
            "Epoch 167/600\n",
            "56/56 [==============================] - 31s 552ms/step - loss: 1.5472 - accuracy: 0.5028\n",
            "Epoch 168/600\n",
            "56/56 [==============================] - 31s 547ms/step - loss: 1.5410 - accuracy: 0.4980\n",
            "Epoch 169/600\n",
            "56/56 [==============================] - 31s 547ms/step - loss: 1.4985 - accuracy: 0.5159\n",
            "Epoch 170/600\n",
            "56/56 [==============================] - 31s 546ms/step - loss: 1.5206 - accuracy: 0.5113\n",
            "Epoch 171/600\n",
            "56/56 [==============================] - 30s 525ms/step - loss: 1.5255 - accuracy: 0.5170\n",
            "Epoch 172/600\n",
            "56/56 [==============================] - 30s 526ms/step - loss: 1.5127 - accuracy: 0.5248\n",
            "Epoch 173/600\n",
            "56/56 [==============================] - 31s 558ms/step - loss: 1.5548 - accuracy: 0.4996\n",
            "Epoch 174/600\n",
            "56/56 [==============================] - 31s 556ms/step - loss: 1.4647 - accuracy: 0.5277\n",
            "Epoch 175/600\n",
            "56/56 [==============================] - 32s 573ms/step - loss: 1.4975 - accuracy: 0.5142\n",
            "Epoch 176/600\n",
            "56/56 [==============================] - 33s 584ms/step - loss: 1.4970 - accuracy: 0.5233\n",
            "Epoch 177/600\n",
            "56/56 [==============================] - 32s 576ms/step - loss: 1.4974 - accuracy: 0.5216\n",
            "Epoch 178/600\n",
            "56/56 [==============================] - 32s 575ms/step - loss: 1.4754 - accuracy: 0.5370\n",
            "Epoch 179/600\n",
            "56/56 [==============================] - 32s 569ms/step - loss: 1.4999 - accuracy: 0.5091\n",
            "Epoch 180/600\n",
            "56/56 [==============================] - 31s 551ms/step - loss: 1.4845 - accuracy: 0.5328\n",
            "Epoch 181/600\n",
            "56/56 [==============================] - 30s 531ms/step - loss: 1.4405 - accuracy: 0.5330\n",
            "Epoch 182/600\n",
            "56/56 [==============================] - 32s 561ms/step - loss: 1.4831 - accuracy: 0.5081\n",
            "Epoch 183/600\n",
            "56/56 [==============================] - 31s 557ms/step - loss: 1.4677 - accuracy: 0.5156\n",
            "Epoch 184/600\n",
            "56/56 [==============================] - 31s 553ms/step - loss: 1.4196 - accuracy: 0.5422\n",
            "Epoch 185/600\n",
            "56/56 [==============================] - 31s 552ms/step - loss: 1.4302 - accuracy: 0.5248\n",
            "Epoch 186/600\n",
            "56/56 [==============================] - 31s 557ms/step - loss: 1.4894 - accuracy: 0.5110\n",
            "Epoch 187/600\n",
            "56/56 [==============================] - 30s 538ms/step - loss: 1.4682 - accuracy: 0.5200\n",
            "Epoch 188/600\n",
            "56/56 [==============================] - 30s 523ms/step - loss: 1.4585 - accuracy: 0.5175\n",
            "Epoch 189/600\n",
            "56/56 [==============================] - 30s 530ms/step - loss: 1.4238 - accuracy: 0.5426\n",
            "Epoch 190/600\n",
            "56/56 [==============================] - 31s 557ms/step - loss: 1.4086 - accuracy: 0.5423\n",
            "Epoch 191/600\n",
            "56/56 [==============================] - 31s 558ms/step - loss: 1.4445 - accuracy: 0.5344\n",
            "Epoch 192/600\n",
            "56/56 [==============================] - 31s 556ms/step - loss: 1.4541 - accuracy: 0.5239\n",
            "Epoch 193/600\n",
            "56/56 [==============================] - 31s 550ms/step - loss: 1.4720 - accuracy: 0.5133\n",
            "Epoch 194/600\n",
            "56/56 [==============================] - 31s 558ms/step - loss: 1.4049 - accuracy: 0.5482\n",
            "Epoch 195/600\n",
            "56/56 [==============================] - 31s 553ms/step - loss: 1.4466 - accuracy: 0.5352\n",
            "Epoch 196/600\n",
            "56/56 [==============================] - 29s 519ms/step - loss: 1.4379 - accuracy: 0.5275\n",
            "Epoch 197/600\n",
            "56/56 [==============================] - 31s 549ms/step - loss: 1.4074 - accuracy: 0.5439\n",
            "Epoch 198/600\n",
            "56/56 [==============================] - 31s 544ms/step - loss: 1.3858 - accuracy: 0.5609\n",
            "Epoch 199/600\n",
            "56/56 [==============================] - 31s 559ms/step - loss: 1.4569 - accuracy: 0.5323\n",
            "Epoch 200/600\n",
            "56/56 [==============================] - 31s 553ms/step - loss: 1.4157 - accuracy: 0.5368\n",
            "Epoch 201/600\n",
            "56/56 [==============================] - 31s 546ms/step - loss: 1.4294 - accuracy: 0.5330\n",
            "Epoch 202/600\n",
            "56/56 [==============================] - 31s 557ms/step - loss: 1.4337 - accuracy: 0.5342\n",
            "Epoch 203/600\n",
            "56/56 [==============================] - 28s 504ms/step - loss: 1.3688 - accuracy: 0.5487\n",
            "Epoch 204/600\n",
            "56/56 [==============================] - 31s 546ms/step - loss: 1.3895 - accuracy: 0.5574\n",
            "Epoch 205/600\n",
            "56/56 [==============================] - 31s 549ms/step - loss: 1.3739 - accuracy: 0.5529\n",
            "Epoch 206/600\n",
            "56/56 [==============================] - 31s 550ms/step - loss: 1.3704 - accuracy: 0.5601\n",
            "Epoch 207/600\n",
            "56/56 [==============================] - 31s 549ms/step - loss: 1.3891 - accuracy: 0.5458\n",
            "Epoch 208/600\n",
            "56/56 [==============================] - 31s 550ms/step - loss: 1.3766 - accuracy: 0.5558\n",
            "Epoch 209/600\n",
            "56/56 [==============================] - 30s 525ms/step - loss: 1.3709 - accuracy: 0.5534\n",
            "Epoch 210/600\n",
            "56/56 [==============================] - 30s 523ms/step - loss: 1.3910 - accuracy: 0.5457\n",
            "Epoch 211/600\n",
            "56/56 [==============================] - 31s 547ms/step - loss: 1.3451 - accuracy: 0.5619\n",
            "Epoch 212/600\n",
            "56/56 [==============================] - 31s 545ms/step - loss: 1.3246 - accuracy: 0.5669\n",
            "Epoch 213/600\n",
            "56/56 [==============================] - 31s 544ms/step - loss: 1.3623 - accuracy: 0.5590\n",
            "Epoch 214/600\n",
            "56/56 [==============================] - 31s 550ms/step - loss: 1.3719 - accuracy: 0.5389\n",
            "Epoch 215/600\n",
            "56/56 [==============================] - 31s 543ms/step - loss: 1.3814 - accuracy: 0.5466\n",
            "Epoch 216/600\n",
            "56/56 [==============================] - 29s 513ms/step - loss: 1.3264 - accuracy: 0.5606\n",
            "Epoch 217/600\n",
            "56/56 [==============================] - 31s 547ms/step - loss: 1.3272 - accuracy: 0.5617\n",
            "Epoch 218/600\n",
            "56/56 [==============================] - 30s 534ms/step - loss: 1.3271 - accuracy: 0.5661\n",
            "Epoch 219/600\n",
            "56/56 [==============================] - 31s 542ms/step - loss: 1.3516 - accuracy: 0.5529\n",
            "Epoch 220/600\n",
            "56/56 [==============================] - 30s 541ms/step - loss: 1.3335 - accuracy: 0.5618\n",
            "Epoch 221/600\n",
            "56/56 [==============================] - 31s 546ms/step - loss: 1.3665 - accuracy: 0.5496\n",
            "Epoch 222/600\n",
            "56/56 [==============================] - 28s 504ms/step - loss: 1.3510 - accuracy: 0.5537\n",
            "Epoch 223/600\n",
            "56/56 [==============================] - 31s 558ms/step - loss: 1.3404 - accuracy: 0.5486\n",
            "Epoch 224/600\n",
            "56/56 [==============================] - 31s 542ms/step - loss: 1.3026 - accuracy: 0.5687\n",
            "Epoch 225/600\n",
            "56/56 [==============================] - 31s 550ms/step - loss: 1.3213 - accuracy: 0.5630\n",
            "Epoch 226/600\n",
            "56/56 [==============================] - 31s 554ms/step - loss: 1.3204 - accuracy: 0.5547\n",
            "Epoch 227/600\n",
            "56/56 [==============================] - 31s 543ms/step - loss: 1.3428 - accuracy: 0.5654\n",
            "Epoch 228/600\n",
            "56/56 [==============================] - 29s 517ms/step - loss: 1.3267 - accuracy: 0.5639\n",
            "Epoch 229/600\n",
            "56/56 [==============================] - 30s 533ms/step - loss: 1.3354 - accuracy: 0.5611\n",
            "Epoch 230/600\n",
            "56/56 [==============================] - 31s 553ms/step - loss: 1.2886 - accuracy: 0.5821\n",
            "Epoch 231/600\n",
            "56/56 [==============================] - 31s 560ms/step - loss: 1.3181 - accuracy: 0.5606\n",
            "Epoch 232/600\n",
            "56/56 [==============================] - 31s 555ms/step - loss: 1.2922 - accuracy: 0.5805\n",
            "Epoch 233/600\n",
            "56/56 [==============================] - 31s 558ms/step - loss: 1.2736 - accuracy: 0.5824\n",
            "Epoch 234/600\n",
            "56/56 [==============================] - 30s 532ms/step - loss: 1.2990 - accuracy: 0.5702\n",
            "Epoch 235/600\n",
            "56/56 [==============================] - 30s 519ms/step - loss: 1.2594 - accuracy: 0.5840\n",
            "Epoch 236/600\n",
            "56/56 [==============================] - 32s 562ms/step - loss: 1.3328 - accuracy: 0.5602\n",
            "Epoch 237/600\n",
            "56/56 [==============================] - 31s 554ms/step - loss: 1.3453 - accuracy: 0.5496\n",
            "Epoch 238/600\n",
            "56/56 [==============================] - 31s 551ms/step - loss: 1.3163 - accuracy: 0.5632\n",
            "Epoch 239/600\n",
            "56/56 [==============================] - 32s 561ms/step - loss: 1.3040 - accuracy: 0.5684\n",
            "Epoch 240/600\n",
            "56/56 [==============================] - 31s 555ms/step - loss: 1.3000 - accuracy: 0.5808\n",
            "Epoch 241/600\n",
            "56/56 [==============================] - 30s 526ms/step - loss: 1.2904 - accuracy: 0.5659\n",
            "Epoch 242/600\n",
            "56/56 [==============================] - 32s 561ms/step - loss: 1.3828 - accuracy: 0.5552\n",
            "Epoch 243/600\n",
            "56/56 [==============================] - 32s 563ms/step - loss: 1.3253 - accuracy: 0.5663\n",
            "Epoch 244/600\n",
            "56/56 [==============================] - 30s 523ms/step - loss: 1.3078 - accuracy: 0.5657\n",
            "Epoch 245/600\n",
            "56/56 [==============================] - 32s 563ms/step - loss: 1.3119 - accuracy: 0.5715\n",
            "Epoch 246/600\n",
            "56/56 [==============================] - 29s 522ms/step - loss: 1.2973 - accuracy: 0.5731\n",
            "Epoch 247/600\n",
            "56/56 [==============================] - 31s 557ms/step - loss: 1.2516 - accuracy: 0.5789\n",
            "Epoch 248/600\n",
            "56/56 [==============================] - 29s 512ms/step - loss: 1.2392 - accuracy: 0.5866\n",
            "Epoch 249/600\n",
            "56/56 [==============================] - 31s 547ms/step - loss: 1.2278 - accuracy: 0.5972\n",
            "Epoch 250/600\n",
            "56/56 [==============================] - 30s 533ms/step - loss: 1.2546 - accuracy: 0.5825\n",
            "Epoch 251/600\n",
            "56/56 [==============================] - 29s 521ms/step - loss: 1.2645 - accuracy: 0.5772\n",
            "Epoch 252/600\n",
            "56/56 [==============================] - 32s 563ms/step - loss: 1.2726 - accuracy: 0.5868\n",
            "Epoch 253/600\n",
            "56/56 [==============================] - 28s 503ms/step - loss: 1.2729 - accuracy: 0.5948\n",
            "Epoch 254/600\n",
            "56/56 [==============================] - 32s 572ms/step - loss: 1.2121 - accuracy: 0.6019\n",
            "Epoch 255/600\n",
            "56/56 [==============================] - 29s 519ms/step - loss: 1.2380 - accuracy: 0.5951\n",
            "Epoch 256/600\n",
            "56/56 [==============================] - 32s 565ms/step - loss: 1.2383 - accuracy: 0.6094\n",
            "Epoch 257/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 1.2200 - accuracy: 0.6000\n",
            "Epoch 258/600\n",
            "56/56 [==============================] - 32s 562ms/step - loss: 1.2180 - accuracy: 0.6001\n",
            "Epoch 259/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 1.2547 - accuracy: 0.5850\n",
            "Epoch 260/600\n",
            "56/56 [==============================] - 29s 510ms/step - loss: 1.2643 - accuracy: 0.5815\n",
            "Epoch 261/600\n",
            "56/56 [==============================] - 31s 553ms/step - loss: 1.2739 - accuracy: 0.5787\n",
            "Epoch 262/600\n",
            "56/56 [==============================] - 29s 511ms/step - loss: 1.2348 - accuracy: 0.5907\n",
            "Epoch 263/600\n",
            "56/56 [==============================] - 32s 562ms/step - loss: 1.2542 - accuracy: 0.5850\n",
            "Epoch 264/600\n",
            "56/56 [==============================] - 29s 517ms/step - loss: 1.3215 - accuracy: 0.5655\n",
            "Epoch 265/600\n",
            "56/56 [==============================] - 32s 565ms/step - loss: 1.2822 - accuracy: 0.5782\n",
            "Epoch 266/600\n",
            "56/56 [==============================] - 29s 507ms/step - loss: 1.2833 - accuracy: 0.5766\n",
            "Epoch 267/600\n",
            "56/56 [==============================] - 32s 564ms/step - loss: 1.2265 - accuracy: 0.5896\n",
            "Epoch 268/600\n",
            "56/56 [==============================] - 29s 517ms/step - loss: 1.2814 - accuracy: 0.5744\n",
            "Epoch 269/600\n",
            "56/56 [==============================] - 30s 533ms/step - loss: 1.2453 - accuracy: 0.5857\n",
            "Epoch 270/600\n",
            "56/56 [==============================] - 31s 543ms/step - loss: 1.2347 - accuracy: 0.5844\n",
            "Epoch 271/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 1.2370 - accuracy: 0.5962\n",
            "Epoch 272/600\n",
            "56/56 [==============================] - 32s 565ms/step - loss: 1.2391 - accuracy: 0.5903\n",
            "Epoch 273/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 1.2463 - accuracy: 0.5845\n",
            "Epoch 274/600\n",
            "56/56 [==============================] - 32s 568ms/step - loss: 1.2488 - accuracy: 0.5809\n",
            "Epoch 275/600\n",
            "56/56 [==============================] - 29s 513ms/step - loss: 1.2153 - accuracy: 0.5912\n",
            "Epoch 276/600\n",
            "56/56 [==============================] - 32s 568ms/step - loss: 1.2502 - accuracy: 0.5903\n",
            "Epoch 277/600\n",
            "56/56 [==============================] - 29s 509ms/step - loss: 1.2290 - accuracy: 0.5910\n",
            "Epoch 278/600\n",
            "56/56 [==============================] - 29s 509ms/step - loss: 1.2123 - accuracy: 0.5889\n",
            "Epoch 279/600\n",
            "56/56 [==============================] - 31s 541ms/step - loss: 1.2515 - accuracy: 0.5863\n",
            "Epoch 280/600\n",
            "56/56 [==============================] - 29s 508ms/step - loss: 1.2493 - accuracy: 0.5826\n",
            "Epoch 281/600\n",
            "56/56 [==============================] - 31s 549ms/step - loss: 1.2201 - accuracy: 0.5973\n",
            "Epoch 282/600\n",
            "56/56 [==============================] - 28s 496ms/step - loss: 1.2362 - accuracy: 0.5944\n",
            "Epoch 283/600\n",
            "56/56 [==============================] - 31s 556ms/step - loss: 1.2534 - accuracy: 0.5818\n",
            "Epoch 284/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 1.2330 - accuracy: 0.5840\n",
            "Epoch 285/600\n",
            "56/56 [==============================] - 31s 559ms/step - loss: 1.2384 - accuracy: 0.5778\n",
            "Epoch 286/600\n",
            "56/56 [==============================] - 28s 505ms/step - loss: 1.1915 - accuracy: 0.5929\n",
            "Epoch 287/600\n",
            "56/56 [==============================] - 29s 512ms/step - loss: 1.2437 - accuracy: 0.5870\n",
            "Epoch 288/600\n",
            "56/56 [==============================] - 32s 565ms/step - loss: 1.2299 - accuracy: 0.5864\n",
            "Epoch 289/600\n",
            "56/56 [==============================] - 29s 518ms/step - loss: 1.2043 - accuracy: 0.5797\n",
            "Epoch 290/600\n",
            "56/56 [==============================] - 32s 563ms/step - loss: 1.1786 - accuracy: 0.6066\n",
            "Epoch 291/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 1.1806 - accuracy: 0.5989\n",
            "Epoch 292/600\n",
            "56/56 [==============================] - 32s 569ms/step - loss: 1.1701 - accuracy: 0.6056\n",
            "Epoch 293/600\n",
            "56/56 [==============================] - 29s 516ms/step - loss: 1.2002 - accuracy: 0.5941\n",
            "Epoch 294/600\n",
            "56/56 [==============================] - 31s 547ms/step - loss: 1.2026 - accuracy: 0.5887\n",
            "Epoch 295/600\n",
            "56/56 [==============================] - 29s 519ms/step - loss: 1.2377 - accuracy: 0.5893\n",
            "Epoch 296/600\n",
            "56/56 [==============================] - 29s 509ms/step - loss: 1.1858 - accuracy: 0.6027\n",
            "Epoch 297/600\n",
            "56/56 [==============================] - 32s 572ms/step - loss: 1.2032 - accuracy: 0.6001\n",
            "Epoch 298/600\n",
            "56/56 [==============================] - 29s 523ms/step - loss: 1.1766 - accuracy: 0.6023\n",
            "Epoch 299/600\n",
            "56/56 [==============================] - 32s 570ms/step - loss: 1.2438 - accuracy: 0.5784\n",
            "Epoch 300/600\n",
            "56/56 [==============================] - 29s 518ms/step - loss: 1.2070 - accuracy: 0.5990\n",
            "Epoch 301/600\n",
            "56/56 [==============================] - 32s 568ms/step - loss: 1.1829 - accuracy: 0.6030\n",
            "Epoch 302/600\n",
            "56/56 [==============================] - 29s 518ms/step - loss: 1.2190 - accuracy: 0.5882\n",
            "Epoch 303/600\n",
            "56/56 [==============================] - 31s 553ms/step - loss: 1.2174 - accuracy: 0.5910\n",
            "Epoch 304/600\n",
            "56/56 [==============================] - 30s 521ms/step - loss: 1.2108 - accuracy: 0.5963\n",
            "Epoch 305/600\n",
            "56/56 [==============================] - 29s 520ms/step - loss: 1.2164 - accuracy: 0.5962\n",
            "Epoch 306/600\n",
            "56/56 [==============================] - 32s 573ms/step - loss: 1.2206 - accuracy: 0.5920\n",
            "Epoch 307/600\n",
            "56/56 [==============================] - 29s 520ms/step - loss: 1.2169 - accuracy: 0.5929\n",
            "Epoch 308/600\n",
            "56/56 [==============================] - 32s 569ms/step - loss: 1.2114 - accuracy: 0.5859\n",
            "Epoch 309/600\n",
            "56/56 [==============================] - 29s 519ms/step - loss: 1.2175 - accuracy: 0.6031\n",
            "Epoch 310/600\n",
            "56/56 [==============================] - 32s 573ms/step - loss: 1.1916 - accuracy: 0.6001\n",
            "Epoch 311/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 1.2203 - accuracy: 0.5903\n",
            "Epoch 312/600\n",
            "56/56 [==============================] - 30s 540ms/step - loss: 1.1744 - accuracy: 0.6110\n",
            "Epoch 313/600\n",
            "56/56 [==============================] - 31s 546ms/step - loss: 1.1850 - accuracy: 0.6023\n",
            "Epoch 314/600\n",
            "56/56 [==============================] - 29s 517ms/step - loss: 1.2097 - accuracy: 0.5844\n",
            "Epoch 315/600\n",
            "56/56 [==============================] - 32s 572ms/step - loss: 1.1567 - accuracy: 0.6090\n",
            "Epoch 316/600\n",
            "56/56 [==============================] - 29s 517ms/step - loss: 1.1847 - accuracy: 0.5963\n",
            "Epoch 317/600\n",
            "56/56 [==============================] - 32s 569ms/step - loss: 1.1891 - accuracy: 0.6078\n",
            "Epoch 318/600\n",
            "56/56 [==============================] - 29s 508ms/step - loss: 1.2014 - accuracy: 0.6038\n",
            "Epoch 319/600\n",
            "56/56 [==============================] - 32s 573ms/step - loss: 1.2065 - accuracy: 0.5869\n",
            "Epoch 320/600\n",
            "56/56 [==============================] - 29s 517ms/step - loss: 1.1879 - accuracy: 0.6024\n",
            "Epoch 321/600\n",
            "56/56 [==============================] - 30s 538ms/step - loss: 1.1725 - accuracy: 0.6061\n",
            "Epoch 322/600\n",
            "56/56 [==============================] - 32s 566ms/step - loss: 1.1979 - accuracy: 0.5930\n",
            "Epoch 323/600\n",
            "56/56 [==============================] - 30s 540ms/step - loss: 1.1468 - accuracy: 0.6124\n",
            "Epoch 324/600\n",
            "56/56 [==============================] - 34s 601ms/step - loss: 1.1814 - accuracy: 0.6180\n",
            "Epoch 325/600\n",
            "56/56 [==============================] - 29s 519ms/step - loss: 1.1606 - accuracy: 0.6022\n",
            "Epoch 326/600\n",
            "56/56 [==============================] - 33s 596ms/step - loss: 1.1627 - accuracy: 0.6066\n",
            "Epoch 327/600\n",
            "56/56 [==============================] - 29s 520ms/step - loss: 1.1640 - accuracy: 0.6010\n",
            "Epoch 328/600\n",
            "56/56 [==============================] - 34s 597ms/step - loss: 1.1431 - accuracy: 0.6063\n",
            "Epoch 329/600\n",
            "56/56 [==============================] - 30s 525ms/step - loss: 1.1628 - accuracy: 0.6057\n",
            "Epoch 330/600\n",
            "56/56 [==============================] - 32s 570ms/step - loss: 1.1527 - accuracy: 0.6101\n",
            "Epoch 331/600\n",
            "56/56 [==============================] - 30s 535ms/step - loss: 1.1391 - accuracy: 0.6120\n",
            "Epoch 332/600\n",
            "56/56 [==============================] - 29s 513ms/step - loss: 1.1622 - accuracy: 0.5994\n",
            "Epoch 333/600\n",
            "56/56 [==============================] - 33s 580ms/step - loss: 1.1731 - accuracy: 0.6019\n",
            "Epoch 334/600\n",
            "56/56 [==============================] - 29s 522ms/step - loss: 1.1447 - accuracy: 0.6172\n",
            "Epoch 335/600\n",
            "56/56 [==============================] - 33s 585ms/step - loss: 1.1582 - accuracy: 0.6070\n",
            "Epoch 336/600\n",
            "56/56 [==============================] - 29s 522ms/step - loss: 1.1599 - accuracy: 0.6152\n",
            "Epoch 337/600\n",
            "56/56 [==============================] - 33s 581ms/step - loss: 1.1519 - accuracy: 0.6167\n",
            "Epoch 338/600\n",
            "56/56 [==============================] - 30s 524ms/step - loss: 1.1896 - accuracy: 0.5958\n",
            "Epoch 339/600\n",
            "56/56 [==============================] - 31s 555ms/step - loss: 1.1371 - accuracy: 0.6141\n",
            "Epoch 340/600\n",
            "56/56 [==============================] - 31s 542ms/step - loss: 1.1346 - accuracy: 0.6190\n",
            "Epoch 341/600\n",
            "56/56 [==============================] - 29s 519ms/step - loss: 1.1905 - accuracy: 0.6015\n",
            "Epoch 342/600\n",
            "56/56 [==============================] - 33s 579ms/step - loss: 1.1623 - accuracy: 0.6136\n",
            "Epoch 343/600\n",
            "56/56 [==============================] - 29s 512ms/step - loss: 1.1257 - accuracy: 0.6109\n",
            "Epoch 344/600\n",
            "56/56 [==============================] - 32s 573ms/step - loss: 1.1738 - accuracy: 0.6049\n",
            "Epoch 345/600\n",
            "56/56 [==============================] - 29s 513ms/step - loss: 1.1512 - accuracy: 0.6023\n",
            "Epoch 346/600\n",
            "56/56 [==============================] - 33s 580ms/step - loss: 1.1545 - accuracy: 0.6034\n",
            "Epoch 347/600\n",
            "56/56 [==============================] - 28s 501ms/step - loss: 1.1757 - accuracy: 0.6054\n",
            "Epoch 348/600\n",
            "56/56 [==============================] - 29s 508ms/step - loss: 1.1199 - accuracy: 0.6264\n",
            "Epoch 349/600\n",
            "56/56 [==============================] - 32s 572ms/step - loss: 1.1908 - accuracy: 0.5978\n",
            "Epoch 350/600\n",
            "56/56 [==============================] - 29s 517ms/step - loss: 1.1371 - accuracy: 0.6127\n",
            "Epoch 351/600\n",
            "56/56 [==============================] - 32s 574ms/step - loss: 1.1256 - accuracy: 0.6248\n",
            "Epoch 352/600\n",
            "56/56 [==============================] - 29s 508ms/step - loss: 1.1192 - accuracy: 0.6199\n",
            "Epoch 353/600\n",
            "56/56 [==============================] - 32s 567ms/step - loss: 1.1803 - accuracy: 0.5928\n",
            "Epoch 354/600\n",
            "56/56 [==============================] - 29s 511ms/step - loss: 1.1661 - accuracy: 0.6101\n",
            "Epoch 355/600\n",
            "56/56 [==============================] - 28s 504ms/step - loss: 1.1946 - accuracy: 0.6007\n",
            "Epoch 356/600\n",
            "56/56 [==============================] - 32s 563ms/step - loss: 1.1544 - accuracy: 0.6021\n",
            "Epoch 357/600\n",
            "56/56 [==============================] - 29s 512ms/step - loss: 1.1608 - accuracy: 0.6054\n",
            "Epoch 358/600\n",
            "56/56 [==============================] - 32s 573ms/step - loss: 1.0989 - accuracy: 0.6220\n",
            "Epoch 359/600\n",
            "56/56 [==============================] - 29s 511ms/step - loss: 1.1162 - accuracy: 0.6246\n",
            "Epoch 360/600\n",
            "56/56 [==============================] - 32s 568ms/step - loss: 1.1099 - accuracy: 0.6215\n",
            "Epoch 361/600\n",
            "56/56 [==============================] - 28s 496ms/step - loss: 1.1157 - accuracy: 0.6204\n",
            "Epoch 362/600\n",
            "56/56 [==============================] - 29s 510ms/step - loss: 1.1419 - accuracy: 0.6048\n",
            "Epoch 363/600\n",
            "56/56 [==============================] - 32s 562ms/step - loss: 1.1944 - accuracy: 0.5940\n",
            "Epoch 364/600\n",
            "56/56 [==============================] - 28s 503ms/step - loss: 1.1291 - accuracy: 0.6184\n",
            "Epoch 365/600\n",
            "56/56 [==============================] - 32s 567ms/step - loss: 1.1197 - accuracy: 0.6060\n",
            "Epoch 366/600\n",
            "56/56 [==============================] - 29s 510ms/step - loss: 1.1195 - accuracy: 0.6219\n",
            "Epoch 367/600\n",
            "56/56 [==============================] - 32s 568ms/step - loss: 1.1238 - accuracy: 0.6145\n",
            "Epoch 368/600\n",
            "56/56 [==============================] - 28s 503ms/step - loss: 1.1477 - accuracy: 0.6003\n",
            "Epoch 369/600\n",
            "56/56 [==============================] - 29s 506ms/step - loss: 1.1336 - accuracy: 0.6078\n",
            "Epoch 370/600\n",
            "56/56 [==============================] - 32s 571ms/step - loss: 1.1155 - accuracy: 0.6253\n",
            "Epoch 371/600\n",
            "56/56 [==============================] - 28s 504ms/step - loss: 1.1187 - accuracy: 0.6116\n",
            "Epoch 372/600\n",
            "56/56 [==============================] - 32s 567ms/step - loss: 1.1420 - accuracy: 0.6147\n",
            "Epoch 373/600\n",
            "56/56 [==============================] - 29s 506ms/step - loss: 1.0899 - accuracy: 0.6248\n",
            "Epoch 374/600\n",
            "56/56 [==============================] - 32s 569ms/step - loss: 1.0645 - accuracy: 0.6390\n",
            "Epoch 375/600\n",
            "56/56 [==============================] - 29s 509ms/step - loss: 1.1369 - accuracy: 0.6167\n",
            "Epoch 376/600\n",
            "56/56 [==============================] - 28s 498ms/step - loss: 1.1216 - accuracy: 0.6226\n",
            "Epoch 377/600\n",
            "56/56 [==============================] - 32s 574ms/step - loss: 1.0904 - accuracy: 0.6271\n",
            "Epoch 378/600\n",
            "56/56 [==============================] - 29s 507ms/step - loss: 1.0841 - accuracy: 0.6292\n",
            "Epoch 379/600\n",
            "56/56 [==============================] - 32s 567ms/step - loss: 1.1471 - accuracy: 0.6243\n",
            "Epoch 380/600\n",
            "56/56 [==============================] - 28s 502ms/step - loss: 1.1547 - accuracy: 0.6091\n",
            "Epoch 381/600\n",
            "56/56 [==============================] - 32s 562ms/step - loss: 1.1041 - accuracy: 0.6176\n",
            "Epoch 382/600\n",
            "56/56 [==============================] - 29s 508ms/step - loss: 1.1907 - accuracy: 0.5836\n",
            "Epoch 383/600\n",
            "56/56 [==============================] - 28s 500ms/step - loss: 1.0971 - accuracy: 0.6287\n",
            "Epoch 384/600\n",
            "56/56 [==============================] - 32s 571ms/step - loss: 1.1125 - accuracy: 0.6138\n",
            "Epoch 385/600\n",
            "56/56 [==============================] - 28s 503ms/step - loss: 1.1058 - accuracy: 0.6211\n",
            "Epoch 386/600\n",
            "56/56 [==============================] - 32s 571ms/step - loss: 1.1493 - accuracy: 0.6060\n",
            "Epoch 387/600\n",
            "56/56 [==============================] - 28s 503ms/step - loss: 1.1101 - accuracy: 0.6195\n",
            "Epoch 388/600\n",
            "56/56 [==============================] - 30s 525ms/step - loss: 1.0928 - accuracy: 0.6242\n",
            "Epoch 389/600\n",
            "56/56 [==============================] - 31s 541ms/step - loss: 1.1145 - accuracy: 0.6146\n",
            "Epoch 390/600\n",
            "56/56 [==============================] - 28s 498ms/step - loss: 1.1399 - accuracy: 0.6094\n",
            "Epoch 391/600\n",
            "56/56 [==============================] - 32s 572ms/step - loss: 1.1234 - accuracy: 0.6263\n",
            "Epoch 392/600\n",
            "56/56 [==============================] - 29s 505ms/step - loss: 1.1019 - accuracy: 0.6302\n",
            "Epoch 393/600\n",
            "56/56 [==============================] - 32s 574ms/step - loss: 1.1012 - accuracy: 0.6232\n",
            "Epoch 394/600\n",
            "56/56 [==============================] - 29s 509ms/step - loss: 1.1488 - accuracy: 0.6130\n",
            "Epoch 395/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 1.1019 - accuracy: 0.6145\n",
            "Epoch 396/600\n",
            "56/56 [==============================] - 32s 563ms/step - loss: 1.0794 - accuracy: 0.6285\n",
            "Epoch 397/600\n",
            "56/56 [==============================] - 29s 511ms/step - loss: 1.1198 - accuracy: 0.6136\n",
            "Epoch 398/600\n",
            "56/56 [==============================] - 32s 576ms/step - loss: 1.0987 - accuracy: 0.6292\n",
            "Epoch 399/600\n",
            "56/56 [==============================] - 28s 502ms/step - loss: 1.1075 - accuracy: 0.6158\n",
            "Epoch 400/600\n",
            "56/56 [==============================] - 33s 579ms/step - loss: 1.1141 - accuracy: 0.6312\n",
            "Epoch 401/600\n",
            "56/56 [==============================] - 29s 510ms/step - loss: 1.1137 - accuracy: 0.6222\n",
            "Epoch 402/600\n",
            "56/56 [==============================] - 29s 513ms/step - loss: 1.0914 - accuracy: 0.6218\n",
            "Epoch 403/600\n",
            "56/56 [==============================] - 33s 577ms/step - loss: 1.1034 - accuracy: 0.6392\n",
            "Epoch 404/600\n",
            "56/56 [==============================] - 29s 517ms/step - loss: 1.1191 - accuracy: 0.6174\n",
            "Epoch 405/600\n",
            "56/56 [==============================] - 33s 588ms/step - loss: 1.1018 - accuracy: 0.6181\n",
            "Epoch 406/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 1.1313 - accuracy: 0.6116\n",
            "Epoch 407/600\n",
            "56/56 [==============================] - 33s 582ms/step - loss: 1.1026 - accuracy: 0.6209\n",
            "Epoch 408/600\n",
            "56/56 [==============================] - 29s 516ms/step - loss: 1.1144 - accuracy: 0.6023\n",
            "Epoch 409/600\n",
            "56/56 [==============================] - 28s 504ms/step - loss: 1.0500 - accuracy: 0.6407\n",
            "Epoch 410/600\n",
            "56/56 [==============================] - 33s 594ms/step - loss: 1.0802 - accuracy: 0.6327\n",
            "Epoch 411/600\n",
            "56/56 [==============================] - 29s 506ms/step - loss: 1.1165 - accuracy: 0.6088\n",
            "Epoch 412/600\n",
            "56/56 [==============================] - 33s 588ms/step - loss: 1.0807 - accuracy: 0.6360\n",
            "Epoch 413/600\n",
            "56/56 [==============================] - 29s 520ms/step - loss: 1.1178 - accuracy: 0.6101\n",
            "Epoch 414/600\n",
            "56/56 [==============================] - 32s 570ms/step - loss: 1.0810 - accuracy: 0.6286\n",
            "Epoch 415/600\n",
            "56/56 [==============================] - 29s 508ms/step - loss: 1.0838 - accuracy: 0.6300\n",
            "Epoch 416/600\n",
            "56/56 [==============================] - 29s 512ms/step - loss: 1.0640 - accuracy: 0.6320\n",
            "Epoch 417/600\n",
            "56/56 [==============================] - 33s 591ms/step - loss: 1.1313 - accuracy: 0.6179\n",
            "Epoch 418/600\n",
            "56/56 [==============================] - 29s 513ms/step - loss: 1.1039 - accuracy: 0.6137\n",
            "Epoch 419/600\n",
            "56/56 [==============================] - 33s 589ms/step - loss: 1.1165 - accuracy: 0.6091\n",
            "Epoch 420/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 1.0919 - accuracy: 0.6312\n",
            "Epoch 421/600\n",
            "56/56 [==============================] - 32s 572ms/step - loss: 1.0972 - accuracy: 0.6280\n",
            "Epoch 422/600\n",
            "56/56 [==============================] - 30s 523ms/step - loss: 1.0779 - accuracy: 0.6369\n",
            "Epoch 423/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 1.0942 - accuracy: 0.6247\n",
            "Epoch 424/600\n",
            "56/56 [==============================] - 33s 585ms/step - loss: 1.1185 - accuracy: 0.6193\n",
            "Epoch 425/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 1.0949 - accuracy: 0.6266\n",
            "Epoch 426/600\n",
            "56/56 [==============================] - 33s 589ms/step - loss: 1.0603 - accuracy: 0.6355\n",
            "Epoch 427/600\n",
            "56/56 [==============================] - 29s 509ms/step - loss: 1.0645 - accuracy: 0.6260\n",
            "Epoch 428/600\n",
            "56/56 [==============================] - 30s 534ms/step - loss: 1.0885 - accuracy: 0.6283\n",
            "Epoch 429/600\n",
            "56/56 [==============================] - 31s 543ms/step - loss: 1.0445 - accuracy: 0.6444\n",
            "Epoch 430/600\n",
            "56/56 [==============================] - 28s 498ms/step - loss: 1.0885 - accuracy: 0.6257\n",
            "Epoch 431/600\n",
            "56/56 [==============================] - 32s 574ms/step - loss: 1.0381 - accuracy: 0.6429\n",
            "Epoch 432/600\n",
            "56/56 [==============================] - 28s 504ms/step - loss: 1.1005 - accuracy: 0.6250\n",
            "Epoch 433/600\n",
            "56/56 [==============================] - 32s 578ms/step - loss: 1.1036 - accuracy: 0.6216\n",
            "Epoch 434/600\n",
            "56/56 [==============================] - 28s 502ms/step - loss: 1.1015 - accuracy: 0.6151\n",
            "Epoch 435/600\n",
            "56/56 [==============================] - 28s 505ms/step - loss: 1.0807 - accuracy: 0.6315\n",
            "Epoch 436/600\n",
            "56/56 [==============================] - 32s 575ms/step - loss: 1.1033 - accuracy: 0.6198\n",
            "Epoch 437/600\n",
            "56/56 [==============================] - 28s 502ms/step - loss: 1.0748 - accuracy: 0.6322\n",
            "Epoch 438/600\n",
            "56/56 [==============================] - 32s 576ms/step - loss: 1.0997 - accuracy: 0.6259\n",
            "Epoch 439/600\n",
            "56/56 [==============================] - 29s 506ms/step - loss: 1.0656 - accuracy: 0.6239\n",
            "Epoch 440/600\n",
            "56/56 [==============================] - 30s 529ms/step - loss: 1.0579 - accuracy: 0.6413\n",
            "Epoch 441/600\n",
            "56/56 [==============================] - 30s 548ms/step - loss: 1.0980 - accuracy: 0.6237\n",
            "Epoch 442/600\n",
            "56/56 [==============================] - 28s 500ms/step - loss: 1.0857 - accuracy: 0.6315\n",
            "Epoch 443/600\n",
            "56/56 [==============================] - 33s 578ms/step - loss: 1.1028 - accuracy: 0.6150\n",
            "Epoch 444/600\n",
            "56/56 [==============================] - 29s 508ms/step - loss: 1.0526 - accuracy: 0.6404\n",
            "Epoch 445/600\n",
            "56/56 [==============================] - 33s 580ms/step - loss: 1.0498 - accuracy: 0.6414\n",
            "Epoch 446/600\n",
            "56/56 [==============================] - 28s 492ms/step - loss: 1.1100 - accuracy: 0.6168\n",
            "Epoch 447/600\n",
            "56/56 [==============================] - 28s 504ms/step - loss: 1.0882 - accuracy: 0.6191\n",
            "Epoch 448/600\n",
            "56/56 [==============================] - 32s 577ms/step - loss: 1.0660 - accuracy: 0.6277\n",
            "Epoch 449/600\n",
            "56/56 [==============================] - 29s 512ms/step - loss: 1.0378 - accuracy: 0.6387\n",
            "Epoch 450/600\n",
            "56/56 [==============================] - 33s 581ms/step - loss: 1.0817 - accuracy: 0.6233\n",
            "Epoch 451/600\n",
            "56/56 [==============================] - 29s 509ms/step - loss: 1.0844 - accuracy: 0.6416\n",
            "Epoch 452/600\n",
            "56/56 [==============================] - 30s 527ms/step - loss: 1.0486 - accuracy: 0.6296\n",
            "Epoch 453/600\n",
            "56/56 [==============================] - 31s 555ms/step - loss: 1.0738 - accuracy: 0.6401\n",
            "Epoch 454/600\n",
            "56/56 [==============================] - 28s 502ms/step - loss: 1.0515 - accuracy: 0.6389\n",
            "Epoch 455/600\n",
            "56/56 [==============================] - 33s 582ms/step - loss: 1.0807 - accuracy: 0.6258\n",
            "Epoch 456/600\n",
            "56/56 [==============================] - 29s 507ms/step - loss: 1.0642 - accuracy: 0.6401\n",
            "Epoch 457/600\n",
            "56/56 [==============================] - 32s 566ms/step - loss: 1.0738 - accuracy: 0.6317\n",
            "Epoch 458/600\n",
            "56/56 [==============================] - 29s 509ms/step - loss: 1.0315 - accuracy: 0.6478\n",
            "Epoch 459/600\n",
            "56/56 [==============================] - 29s 507ms/step - loss: 1.0816 - accuracy: 0.6163\n",
            "Epoch 460/600\n",
            "56/56 [==============================] - 33s 590ms/step - loss: 1.0390 - accuracy: 0.6354\n",
            "Epoch 461/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 1.0437 - accuracy: 0.6290\n",
            "Epoch 462/600\n",
            "56/56 [==============================] - 33s 581ms/step - loss: 1.0666 - accuracy: 0.6289\n",
            "Epoch 463/600\n",
            "56/56 [==============================] - 28s 495ms/step - loss: 1.0705 - accuracy: 0.6293\n",
            "Epoch 464/600\n",
            "56/56 [==============================] - 28s 505ms/step - loss: 1.0553 - accuracy: 0.6337\n",
            "Epoch 465/600\n",
            "56/56 [==============================] - 32s 568ms/step - loss: 1.0573 - accuracy: 0.6338\n",
            "Epoch 466/600\n",
            "56/56 [==============================] - 28s 500ms/step - loss: 1.0786 - accuracy: 0.6313\n",
            "Epoch 467/600\n",
            "56/56 [==============================] - 32s 575ms/step - loss: 1.0596 - accuracy: 0.6363\n",
            "Epoch 468/600\n",
            "56/56 [==============================] - 28s 502ms/step - loss: 1.0557 - accuracy: 0.6465\n",
            "Epoch 469/600\n",
            "56/56 [==============================] - 30s 540ms/step - loss: 1.0657 - accuracy: 0.6338\n",
            "Epoch 470/600\n",
            "56/56 [==============================] - 30s 535ms/step - loss: 1.0211 - accuracy: 0.6558\n",
            "Epoch 471/600\n",
            "56/56 [==============================] - 28s 498ms/step - loss: 1.0463 - accuracy: 0.6379\n",
            "Epoch 472/600\n",
            "56/56 [==============================] - 33s 584ms/step - loss: 1.0471 - accuracy: 0.6428\n",
            "Epoch 473/600\n",
            "56/56 [==============================] - 28s 502ms/step - loss: 1.0340 - accuracy: 0.6405\n",
            "Epoch 474/600\n",
            "56/56 [==============================] - 32s 577ms/step - loss: 1.0341 - accuracy: 0.6405\n",
            "Epoch 475/600\n",
            "56/56 [==============================] - 28s 500ms/step - loss: 1.0762 - accuracy: 0.6234\n",
            "Epoch 476/600\n",
            "56/56 [==============================] - 29s 510ms/step - loss: 1.0717 - accuracy: 0.6333\n",
            "Epoch 477/600\n",
            "56/56 [==============================] - 33s 592ms/step - loss: 1.0491 - accuracy: 0.6476\n",
            "Epoch 478/600\n",
            "56/56 [==============================] - 29s 511ms/step - loss: 1.0674 - accuracy: 0.6409\n",
            "Epoch 479/600\n",
            "56/56 [==============================] - 34s 601ms/step - loss: 1.0120 - accuracy: 0.6529\n",
            "Epoch 480/600\n",
            "56/56 [==============================] - 29s 510ms/step - loss: 1.0749 - accuracy: 0.6296\n",
            "Epoch 481/600\n",
            "56/56 [==============================] - 31s 542ms/step - loss: 1.0365 - accuracy: 0.6373\n",
            "Epoch 482/600\n",
            "56/56 [==============================] - 32s 565ms/step - loss: 1.0528 - accuracy: 0.6418\n",
            "Epoch 483/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 1.0304 - accuracy: 0.6489\n",
            "Epoch 484/600\n",
            "56/56 [==============================] - 34s 616ms/step - loss: 1.0158 - accuracy: 0.6463\n",
            "Epoch 485/600\n",
            "56/56 [==============================] - 29s 507ms/step - loss: 1.0422 - accuracy: 0.6380\n",
            "Epoch 486/600\n",
            "56/56 [==============================] - 34s 597ms/step - loss: 1.0646 - accuracy: 0.6344\n",
            "Epoch 487/600\n",
            "56/56 [==============================] - 30s 523ms/step - loss: 1.0393 - accuracy: 0.6445\n",
            "Epoch 488/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 1.0402 - accuracy: 0.6429\n",
            "Epoch 489/600\n",
            "56/56 [==============================] - 34s 603ms/step - loss: 1.0907 - accuracy: 0.6302\n",
            "Epoch 490/600\n",
            "56/56 [==============================] - 30s 527ms/step - loss: 1.0444 - accuracy: 0.6412\n",
            "Epoch 491/600\n",
            "56/56 [==============================] - 35s 622ms/step - loss: 1.0592 - accuracy: 0.6283\n",
            "Epoch 492/600\n",
            "56/56 [==============================] - 30s 529ms/step - loss: 1.0482 - accuracy: 0.6433\n",
            "Epoch 493/600\n",
            "56/56 [==============================] - 29s 519ms/step - loss: 1.0633 - accuracy: 0.6341\n",
            "Epoch 494/600\n",
            "56/56 [==============================] - 30s 527ms/step - loss: 1.0222 - accuracy: 0.6487\n",
            "Epoch 495/600\n",
            "56/56 [==============================] - 33s 586ms/step - loss: 1.0638 - accuracy: 0.6361\n",
            "Epoch 496/600\n",
            "56/56 [==============================] - 31s 543ms/step - loss: 1.0200 - accuracy: 0.6401\n",
            "Epoch 497/600\n",
            "56/56 [==============================] - 29s 519ms/step - loss: 1.0242 - accuracy: 0.6457\n",
            "Epoch 498/600\n",
            "56/56 [==============================] - 29s 521ms/step - loss: 1.0663 - accuracy: 0.6278\n",
            "Epoch 499/600\n",
            "56/56 [==============================] - 29s 523ms/step - loss: 1.0267 - accuracy: 0.6510\n",
            "Epoch 500/600\n",
            "56/56 [==============================] - 34s 614ms/step - loss: 1.0079 - accuracy: 0.6476\n",
            "Epoch 501/600\n",
            "56/56 [==============================] - 30s 533ms/step - loss: 1.0452 - accuracy: 0.6417\n",
            "Epoch 502/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 1.0370 - accuracy: 0.6327\n",
            "Epoch 503/600\n",
            "56/56 [==============================] - 30s 528ms/step - loss: 1.0464 - accuracy: 0.6333\n",
            "Epoch 504/600\n",
            "56/56 [==============================] - 35s 628ms/step - loss: 0.9901 - accuracy: 0.6556\n",
            "Epoch 505/600\n",
            "56/56 [==============================] - 30s 524ms/step - loss: 1.0474 - accuracy: 0.6328\n",
            "Epoch 506/600\n",
            "56/56 [==============================] - 32s 574ms/step - loss: 1.0432 - accuracy: 0.6490\n",
            "Epoch 507/600\n",
            "56/56 [==============================] - 30s 535ms/step - loss: 1.0433 - accuracy: 0.6406\n",
            "Epoch 508/600\n",
            "56/56 [==============================] - 34s 605ms/step - loss: 1.0279 - accuracy: 0.6427\n",
            "Epoch 509/600\n",
            "56/56 [==============================] - 30s 535ms/step - loss: 1.0107 - accuracy: 0.6426\n",
            "Epoch 510/600\n",
            "56/56 [==============================] - 30s 533ms/step - loss: 1.0090 - accuracy: 0.6585\n",
            "Epoch 511/600\n",
            "56/56 [==============================] - 30s 523ms/step - loss: 1.0498 - accuracy: 0.6405\n",
            "Epoch 512/600\n",
            "56/56 [==============================] - 29s 521ms/step - loss: 1.0222 - accuracy: 0.6532\n",
            "Epoch 513/600\n",
            "56/56 [==============================] - 34s 601ms/step - loss: 1.0184 - accuracy: 0.6473\n",
            "Epoch 514/600\n",
            "56/56 [==============================] - 29s 512ms/step - loss: 0.9939 - accuracy: 0.6545\n",
            "Epoch 515/600\n",
            "56/56 [==============================] - 29s 508ms/step - loss: 0.9873 - accuracy: 0.6669\n",
            "Epoch 516/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 1.0111 - accuracy: 0.6694\n",
            "Epoch 517/600\n",
            "56/56 [==============================] - 34s 604ms/step - loss: 1.0740 - accuracy: 0.6356\n",
            "Epoch 518/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 1.0414 - accuracy: 0.6376\n",
            "Epoch 519/600\n",
            "56/56 [==============================] - 29s 510ms/step - loss: 1.0667 - accuracy: 0.6413\n",
            "Epoch 520/600\n",
            "56/56 [==============================] - 29s 520ms/step - loss: 1.0059 - accuracy: 0.6418\n",
            "Epoch 521/600\n",
            "56/56 [==============================] - 29s 520ms/step - loss: 1.0038 - accuracy: 0.6541\n",
            "Epoch 522/600\n",
            "56/56 [==============================] - 34s 594ms/step - loss: 1.0226 - accuracy: 0.6510\n",
            "Epoch 523/600\n",
            "56/56 [==============================] - 29s 509ms/step - loss: 1.0047 - accuracy: 0.6381\n",
            "Epoch 524/600\n",
            "56/56 [==============================] - 29s 510ms/step - loss: 1.0351 - accuracy: 0.6512\n",
            "Epoch 525/600\n",
            "56/56 [==============================] - 29s 513ms/step - loss: 0.9832 - accuracy: 0.6644\n",
            "Epoch 526/600\n",
            "56/56 [==============================] - 34s 613ms/step - loss: 1.0124 - accuracy: 0.6509\n",
            "Epoch 527/600\n",
            "56/56 [==============================] - 29s 512ms/step - loss: 1.0043 - accuracy: 0.6554\n",
            "Epoch 528/600\n",
            "56/56 [==============================] - 29s 508ms/step - loss: 0.9871 - accuracy: 0.6584\n",
            "Epoch 529/600\n",
            "56/56 [==============================] - 29s 516ms/step - loss: 0.9744 - accuracy: 0.6737\n",
            "Epoch 530/600\n",
            "56/56 [==============================] - 28s 504ms/step - loss: 1.0055 - accuracy: 0.6513\n",
            "Epoch 531/600\n",
            "56/56 [==============================] - 34s 591ms/step - loss: 0.9668 - accuracy: 0.6621\n",
            "Epoch 532/600\n",
            "56/56 [==============================] - 29s 510ms/step - loss: 1.0318 - accuracy: 0.6311\n",
            "Epoch 533/600\n",
            "56/56 [==============================] - 29s 507ms/step - loss: 0.9911 - accuracy: 0.6584\n",
            "Epoch 534/600\n",
            "56/56 [==============================] - 29s 509ms/step - loss: 1.0064 - accuracy: 0.6491\n",
            "Epoch 535/600\n",
            "56/56 [==============================] - 34s 606ms/step - loss: 1.0050 - accuracy: 0.6421\n",
            "Epoch 536/600\n",
            "56/56 [==============================] - 29s 511ms/step - loss: 0.9938 - accuracy: 0.6646\n",
            "Epoch 537/600\n",
            "56/56 [==============================] - 29s 518ms/step - loss: 1.0351 - accuracy: 0.6544\n",
            "Epoch 538/600\n",
            "56/56 [==============================] - 29s 518ms/step - loss: 1.0085 - accuracy: 0.6509\n",
            "Epoch 539/600\n",
            "56/56 [==============================] - 29s 520ms/step - loss: 0.9734 - accuracy: 0.6539\n",
            "Epoch 540/600\n",
            "56/56 [==============================] - 33s 596ms/step - loss: 0.9723 - accuracy: 0.6680\n",
            "Epoch 541/600\n",
            "56/56 [==============================] - 29s 511ms/step - loss: 0.9954 - accuracy: 0.6600\n",
            "Epoch 542/600\n",
            "56/56 [==============================] - 29s 518ms/step - loss: 0.9997 - accuracy: 0.6515\n",
            "Epoch 543/600\n",
            "56/56 [==============================] - 28s 504ms/step - loss: 0.9983 - accuracy: 0.6450\n",
            "Epoch 544/600\n",
            "56/56 [==============================] - 34s 606ms/step - loss: 1.0132 - accuracy: 0.6415\n",
            "Epoch 545/600\n",
            "56/56 [==============================] - 29s 507ms/step - loss: 0.9938 - accuracy: 0.6540\n",
            "Epoch 546/600\n",
            "56/56 [==============================] - 30s 526ms/step - loss: 1.0071 - accuracy: 0.6410\n",
            "Epoch 547/600\n",
            "56/56 [==============================] - 29s 511ms/step - loss: 1.0134 - accuracy: 0.6520\n",
            "Epoch 548/600\n",
            "56/56 [==============================] - 31s 545ms/step - loss: 1.0228 - accuracy: 0.6409\n",
            "Epoch 549/600\n",
            "56/56 [==============================] - 33s 586ms/step - loss: 1.0559 - accuracy: 0.6396\n",
            "Epoch 550/600\n",
            "56/56 [==============================] - 29s 518ms/step - loss: 1.0164 - accuracy: 0.6489\n",
            "Epoch 551/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 1.0121 - accuracy: 0.6472\n",
            "Epoch 552/600\n",
            "56/56 [==============================] - 29s 519ms/step - loss: 1.0083 - accuracy: 0.6461\n",
            "Epoch 553/600\n",
            "56/56 [==============================] - 35s 614ms/step - loss: 1.0163 - accuracy: 0.6591\n",
            "Epoch 554/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 1.0352 - accuracy: 0.6401\n",
            "Epoch 555/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 0.9910 - accuracy: 0.6658\n",
            "Epoch 556/600\n",
            "56/56 [==============================] - 28s 505ms/step - loss: 1.0216 - accuracy: 0.6429\n",
            "Epoch 557/600\n",
            "56/56 [==============================] - 31s 543ms/step - loss: 1.0219 - accuracy: 0.6458\n",
            "Epoch 558/600\n",
            "56/56 [==============================] - 33s 580ms/step - loss: 1.0110 - accuracy: 0.6464\n",
            "Epoch 559/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 1.0186 - accuracy: 0.6476\n",
            "Epoch 560/600\n",
            "56/56 [==============================] - 29s 507ms/step - loss: 1.0047 - accuracy: 0.6574\n",
            "Epoch 561/600\n",
            "56/56 [==============================] - 29s 512ms/step - loss: 1.0347 - accuracy: 0.6504\n",
            "Epoch 562/600\n",
            "56/56 [==============================] - 34s 604ms/step - loss: 1.0286 - accuracy: 0.6353\n",
            "Epoch 563/600\n",
            "56/56 [==============================] - 29s 513ms/step - loss: 1.0222 - accuracy: 0.6529\n",
            "Epoch 564/600\n",
            "56/56 [==============================] - 29s 508ms/step - loss: 0.9731 - accuracy: 0.6603\n",
            "Epoch 565/600\n",
            "56/56 [==============================] - 29s 511ms/step - loss: 0.9915 - accuracy: 0.6588\n",
            "Epoch 566/600\n",
            "56/56 [==============================] - 29s 518ms/step - loss: 0.9874 - accuracy: 0.6587\n",
            "Epoch 567/600\n",
            "56/56 [==============================] - 33s 588ms/step - loss: 1.0135 - accuracy: 0.6393\n",
            "Epoch 568/600\n",
            "56/56 [==============================] - 29s 507ms/step - loss: 0.9728 - accuracy: 0.6644\n",
            "Epoch 569/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 0.9653 - accuracy: 0.6578\n",
            "Epoch 570/600\n",
            "56/56 [==============================] - 29s 518ms/step - loss: 0.9348 - accuracy: 0.6778\n",
            "Epoch 571/600\n",
            "56/56 [==============================] - 34s 609ms/step - loss: 0.9675 - accuracy: 0.6676\n",
            "Epoch 572/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 0.9328 - accuracy: 0.6740\n",
            "Epoch 573/600\n",
            "56/56 [==============================] - 29s 513ms/step - loss: 0.9984 - accuracy: 0.6548\n",
            "Epoch 574/600\n",
            "56/56 [==============================] - 29s 513ms/step - loss: 0.9459 - accuracy: 0.6729\n",
            "Epoch 575/600\n",
            "56/56 [==============================] - 29s 518ms/step - loss: 0.9749 - accuracy: 0.6639\n",
            "Epoch 576/600\n",
            "56/56 [==============================] - 33s 590ms/step - loss: 0.9804 - accuracy: 0.6617\n",
            "Epoch 577/600\n",
            "56/56 [==============================] - 29s 513ms/step - loss: 0.9846 - accuracy: 0.6541\n",
            "Epoch 578/600\n",
            "56/56 [==============================] - 29s 521ms/step - loss: 0.9989 - accuracy: 0.6566\n",
            "Epoch 579/600\n",
            "56/56 [==============================] - 30s 526ms/step - loss: 1.0128 - accuracy: 0.6492\n",
            "Epoch 580/600\n",
            "56/56 [==============================] - 35s 615ms/step - loss: 0.9907 - accuracy: 0.6581\n",
            "Epoch 581/600\n",
            "56/56 [==============================] - 29s 508ms/step - loss: 0.9602 - accuracy: 0.6624\n",
            "Epoch 582/600\n",
            "56/56 [==============================] - 29s 511ms/step - loss: 0.9973 - accuracy: 0.6508\n",
            "Epoch 583/600\n",
            "56/56 [==============================] - 28s 505ms/step - loss: 0.9804 - accuracy: 0.6534\n",
            "Epoch 584/600\n",
            "56/56 [==============================] - 29s 525ms/step - loss: 0.9546 - accuracy: 0.6689\n",
            "Epoch 585/600\n",
            "56/56 [==============================] - 34s 602ms/step - loss: 0.9944 - accuracy: 0.6465\n",
            "Epoch 586/600\n",
            "56/56 [==============================] - 29s 513ms/step - loss: 0.9638 - accuracy: 0.6678\n",
            "Epoch 587/600\n",
            "56/56 [==============================] - 30s 524ms/step - loss: 0.9938 - accuracy: 0.6519\n",
            "Epoch 588/600\n",
            "56/56 [==============================] - 29s 514ms/step - loss: 0.9620 - accuracy: 0.6749\n",
            "Epoch 589/600\n",
            "56/56 [==============================] - 36s 642ms/step - loss: 1.0214 - accuracy: 0.6488\n",
            "Epoch 590/600\n",
            "56/56 [==============================] - 29s 523ms/step - loss: 1.0067 - accuracy: 0.6495\n",
            "Epoch 591/600\n",
            "56/56 [==============================] - 29s 515ms/step - loss: 0.9478 - accuracy: 0.6676\n",
            "Epoch 592/600\n",
            "56/56 [==============================] - 29s 522ms/step - loss: 0.9791 - accuracy: 0.6636\n",
            "Epoch 593/600\n",
            "56/56 [==============================] - 31s 552ms/step - loss: 0.9755 - accuracy: 0.6654\n",
            "Epoch 594/600\n",
            "56/56 [==============================] - 34s 599ms/step - loss: 0.9598 - accuracy: 0.6699\n",
            "Epoch 595/600\n",
            "56/56 [==============================] - 30s 540ms/step - loss: 0.9949 - accuracy: 0.6527\n",
            "Epoch 596/600\n",
            "56/56 [==============================] - 29s 520ms/step - loss: 1.0102 - accuracy: 0.6509\n",
            "Epoch 597/600\n",
            "56/56 [==============================] - 29s 521ms/step - loss: 0.9750 - accuracy: 0.6613\n",
            "Epoch 598/600\n",
            "56/56 [==============================] - 35s 624ms/step - loss: 1.0238 - accuracy: 0.6520\n",
            "Epoch 599/600\n",
            "56/56 [==============================] - 29s 510ms/step - loss: 0.9738 - accuracy: 0.6685\n",
            "Epoch 600/600\n",
            "56/56 [==============================] - 29s 503ms/step - loss: 0.9988 - accuracy: 0.6521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-28T18:42:38.683012Z",
          "iopub.status.busy": "2021-09-28T18:42:38.682161Z",
          "iopub.status.idle": "2021-09-28T18:42:39.919859Z",
          "shell.execute_reply": "2021-09-28T18:42:39.918884Z",
          "shell.execute_reply.started": "2021-09-28T13:34:03.845303Z"
        },
        "papermill": {
          "duration": 10.867289,
          "end_time": "2021-09-28T18:42:39.920012",
          "exception": false,
          "start_time": "2021-09-28T18:42:29.052723",
          "status": "completed"
        },
        "tags": [],
        "id": "aee76059"
      },
      "source": [
        "resmodel.save_weights(\"/kaggle/working/task2resnet50(w).h5\")\n",
        "resmodel.save(\"/kaggle/working/task2resnet50(m).h5\")     #Saving best model and its corresponding weights "
      ],
      "id": "aee76059",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 9.919785,
          "end_time": "2021-09-28T18:42:59.366504",
          "exception": false,
          "start_time": "2021-09-28T18:42:49.446719",
          "status": "completed"
        },
        "tags": [],
        "id": "1eeae9ad"
      },
      "source": [
        ""
      ],
      "id": "1eeae9ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 9.685293,
          "end_time": "2021-09-28T18:43:18.469148",
          "exception": false,
          "start_time": "2021-09-28T18:43:08.783855",
          "status": "completed"
        },
        "tags": [],
        "id": "ded1ce89"
      },
      "source": [
        ""
      ],
      "id": "ded1ce89",
      "execution_count": null,
      "outputs": []
    }
  ]
}